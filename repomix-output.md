This file is a merged representation of the entire codebase, combined into a single document by Repomix.
The content has been processed where content has been compressed (code blocks are separated by â‹®---- delimiter).

# File Summary

## Purpose
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

## File Format
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  a. A header with the file path (## File: path/to/file)
  b. The full contents of the file in a code block

## Usage Guidelines
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

## Notes
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Content has been compressed - code blocks are separated by â‹®---- delimiter
- Files are sorted by Git change count (files with more changes are at the bottom)

# Directory Structure
```
__init__.py
agent_display_console.py
command_converter.py
context_helpers.py
file_logger.py
llm_client.py
miniOR.py
output_manager.py
web_ui.py
```

# Files

## File: __init__.py
````python
"""Utility functions and classes used throughout Slaze."""
â‹®----
__all__ = []
````

## File: agent_display_console.py
````python
class AgentDisplayConsole
â‹®----
def __init__(self)
â‹®----
# Configure console for Windows Unicode support
if os.name == 'nt':  # Windows
# Set environment variables for UTF-8 support
â‹®----
# Try to set Windows console to UTF-8 mode
â‹®----
# Enable UTF-8 mode on Windows
â‹®----
# Reconfigure stdout/stderr for UTF-8 (Python 3.7+)
â‹®----
# Initialize Rich console with safe settings for Windows
â‹®----
def add_message(self, msg_type, content)
â‹®----
# Use safer characters for Windows compatibility
â‹®----
async def wait_for_user_input(self, prompt_message=">> Your input: ")
â‹®----
async def select_prompt_console(self)
â‹®----
options = {}
prompt_files = sorted([f for f in PROMPTS_DIR.iterdir() if f.is_file() and f.suffix == '.md'])
â‹®----
prompt_lines = []
â‹®----
create_new_option_num = len(options) + 1
â‹®----
choice = IntPrompt.ask("Enter your choice", choices=[str(i) for i in range(1, create_new_option_num + 1)])
â‹®----
prompt_path = options[str(choice)]
task = prompt_path.read_text(encoding="utf-8")
prompt_name = prompt_path.stem
â‹®----
new_lines = []
â‹®----
line = await self.wait_for_user_input("")
â‹®----
task = "\n".join(new_lines)
â‹®----
new_filename_input = Prompt.ask("Enter a filename for the new prompt", default="custom_prompt")
filename_stem = new_filename_input.strip().replace(" ", "_").replace(".md", "")
prompt_name = filename_stem
new_prompt_path = PROMPTS_DIR / f"{filename_stem}.md"
new_prompt_lines = []
â‹®----
task = "\n".join(new_prompt_lines)
â‹®----
task = ""
â‹®----
# Configure repository directory for this prompt
base_repo_dir = Path(get_constant("TOP_LEVEL_DIR")) / "repo"
repo_dir = base_repo_dir / prompt_name
â‹®----
async def confirm_tool_call(self, tool_name: str, args: dict, schema: dict) -> dict | None
â‹®----
"""Display tool call parameters and allow the user to edit them."""
â‹®----
updated_args = dict(args)
properties = schema.get("properties", {}) if schema else {}
â‹®----
current_val = args.get(param, "")
default_str = str(current_val) if current_val is not None else ""
user_val = Prompt.ask(param, default=default_str)
â‹®----
pinfo = properties.get(param, {})
````

## File: command_converter.py
````python
logger = logging.getLogger(__name__)
â‹®----
class CommandConverter
â‹®----
"""
    LLM-based command converter that transforms bash commands to be appropriate
    for the current system environment.
    """
â‹®----
def __init__(self)
â‹®----
def _get_system_info(self) -> Dict[str, Any]
â‹®----
"""Gather system information for command conversion context."""
â‹®----
def _build_conversion_prompt(self) -> str
â‹®----
"""Build the system prompt for command conversion."""
os_name = self.system_info['os_name']
â‹®----
# Build OS-specific examples and rules
â‹®----
examples = """EXAMPLES:
â‹®----
rules = f"""RULES:
â‹®----
async def convert_command(self, original_command: str) -> str
â‹®----
"""
        Convert a command using LLM to be appropriate for the current system.
        
        Args:
            original_command: The original bash command to convert
            
        Returns:
            The converted command appropriate for the current system
        """
â‹®----
# Get the model from config
model = get_constant("MAIN_MODEL", "anthropic/claude-sonnet-4")
â‹®----
# Prepare the conversion request
converted_command = await self._call_llm(model, original_command)
â‹®----
# Validate and clean the response
cleaned_command = self._clean_response(converted_command)
â‹®----
# Fallback to original command if conversion fails
â‹®----
async def _call_llm(self, model: str, command: str) -> str
â‹®----
"""
        Call the LLM API to convert the command.
        
        Args:
            model: The model to use for conversion
            command: The original command
            
        Returns:
            The LLM response containing the converted command
        """
# Prepare the messages for the LLM
messages = [
â‹®----
# Create LLM client and call it
client = create_llm_client(model)
â‹®----
max_tokens=200,  # Keep response short
temperature=0.1  # Low temperature for consistent output
â‹®----
def _clean_response(self, response: str) -> str
â‹®----
"""
        Clean the LLM response to extract just the command.
        
        Args:
            response: The raw LLM response
            
        Returns:
            The cleaned command string
        """
# Remove any markdown code blocks
response = re.sub(r'^```.*?\n|```$', '', response, flags=re.MULTILINE)
â‹®----
# Remove leading/trailing whitespace
response = response.strip()
â‹®----
# Split by lines and take the first non-empty line
lines = [line.strip() for line in response.split('\n') if line.strip()]
â‹®----
command = lines[0]
â‹®----
# Basic validation - ensure it looks like a command
if not command or len(command) > 1000:  # Reasonable length limit
â‹®----
# Global instance for reuse
_converter_instance: Optional[CommandConverter] = None
â‹®----
async def convert_command_for_system(original_command: str) -> str
â‹®----
"""
    Convert a bash command to be appropriate for the current system.
    
    Args:
        original_command: The original bash command
        
    Returns:
        The converted command appropriate for the current system
    """
â‹®----
_converter_instance = CommandConverter()
````

## File: context_helpers.py
````python
# from config import write_to_file # Removed as it was for ic
# Removed: from load_constants import *
from config import MAIN_MODEL, get_constant, googlepro # Import get_constant
â‹®----
# from icecream import ic # Removed
# from rich import print as rr # Removed
â‹®----
# ic.configureOutput(includeContext=True, outputFunction=write_to_file) # Removed
â‹®----
logger = logging.getLogger(__name__)
â‹®----
QUICK_SUMMARIES = []
â‹®----
def format_messages_to_restart(messages)
â‹®----
"""
    Format a list of messages into a formatted string.
    """
â‹®----
output_pieces = []
â‹®----
def format_messages_to_string(messages)
â‹®----
"""Return a human readable string for a list of messages."""
â‹®----
def _val(obj, key, default=None)
â‹®----
role = msg.get("role", "unknown").upper()
â‹®----
name = _val(_val(tc, "function"), "name")
args = _val(_val(tc, "function"), "arguments")
tc_id = _val(tc, "id")
â‹®----
parsed = json.loads(args) if isinstance(args, str) else args
formatted = json.dumps(parsed, indent=2)
â‹®----
formatted = str(args)
â‹®----
content = msg.get("content")
â‹®----
btype = block.get("type")
â‹®----
inp = block["input"]
â‹®----
"""
    Summarize the most recent messages.
    """
â‹®----
OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY")
sum_client = OpenAI(
all_summaries = get_all_summaries()
model = MAIN_MODEL
conversation_text = ""
â‹®----
role = msg["role"].upper()
â‹®----
content = block.get("text", "")
â‹®----
content = (
â‹®----
content = item.get("text", "")
â‹®----
content = msg["content"]
â‹®----
logger.debug(f"conversation_text for summary: {conversation_text[:500]}...") # Log snippet
summary_prompt = f"""Please provide your response in a concise markdown format with short statements that document what happened. Structure your response as a list with clear labels for each step, such as:
response = sum_client.chat.completions.create(
â‹®----
max_tokens=get_constant("MAX_SUMMARY_TOKENS", 4000) # Use get_constant
â‹®----
# Add error handling for response
â‹®----
error_msg = "Error: No valid response received from summary API"
# print(response) # Replaced by logger
â‹®----
summary = response.choices[0].message.content
â‹®----
# Check if summary is None or empty
â‹®----
error_msg = "Error: Empty summary received from API"
â‹®----
logger.debug(f"Generated summary: {summary[:500]}...") # Log snippet
â‹®----
error_msg = f"Error generating summary: {str(e)}"
â‹®----
def filter_messages(messages: List[Dict]) -> List[Dict]
â‹®----
"""
    Keep only messages with role 'user' or 'assistant'.
    Also keep any tool_result messages that contain errors.
    """
keep_roles = {"user", "assistant"}
filtered = []
â‹®----
# Check if any text in the tool result indicates an error
text = ""
â‹®----
def extract_text_from_content(content: Any) -> str
â‹®----
text_parts = []
â‹®----
def truncate_message_content(content: Any, max_length: int = 150_000) -> Any
â‹®----
def add_summary(summary: str) -> None
â‹®----
"""Add a new summary to the global list with timestamp and log it to a file."""
stripped_summary = summary.strip()
â‹®----
summary_file_path = Path(get_constant("SUMMARY_FILE"))
â‹®----
timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
log_entry = f"\n--------------------\n[{timestamp}]\n{stripped_summary}\n--------------------\n"
â‹®----
def get_all_summaries() -> str
â‹®----
"""Combine all summaries into a chronological narrative."""
â‹®----
combined = "\n"
â‹®----
async def reorganize_context(messages: List[Dict[str, Any]], summary: str) -> str
â‹®----
"""Reorganize the context by filtering and summarizing messages."""
â‹®----
# Look for tool results related to image generation
image_generation_results = []
â‹®----
# Track image generation results
â‹®----
# Add special section for image generation if we found any
â‹®----
logger.debug(f"Conversation text for reorganize_context: {conversation_text[:500]}...") # Log snippet
summary_prompt = f"""I need a summary of completed steps and next steps for a project that is ALREADY IN PROGRESS.
â‹®----
logger.debug(f"Reorganized context summary: {summary[:500]}...") # Log snippet
â‹®----
start_tag = "<COMPLETED>"
end_tag = "</COMPLETED>"
â‹®----
completed_items = summary[
â‹®----
completed_items = "No completed items found."
â‹®----
start_tag = "<NEXT_STEPS>"
end_tag = "</NEXT_STEPS>"
â‹®----
steps = summary[
â‹®----
steps = "No steps found."
â‹®----
# Return default values in case of error
â‹®----
"""
    Create a combined context string by filtering and (if needed) summarizing messages
    and appending current file contents.
    """
filtered = filter_messages(messages)
summary = get_all_summaries() # This is a local function in context_helpers
â‹®----
file_contents = aggregate_file_states()
â‹®----
file_contents = (
â‹®----
# Get code skeletons
â‹®----
code_skeletons = get_all_current_skeleton()
current_code = get_all_current_code()
# The logic is if there is code, then supply that, if not then supply the skeletons, if there is no code or skeletons, then say there are no code skeletons
â‹®----
code_skeletons = current_code
â‹®----
code_skeletons = "No code skeletons available."
â‹®----
# Extract information about images generated
images_info = ""
â‹®----
images_section = file_contents.split("## Generated Images:")[1]
â‹®----
images_section = images_section.split("##")[0]
images_info = "## Generated Images:\n" + images_section.strip()
â‹®----
# call the LLM and pass it all current messages then the task and ask it to give an updated version of the task
prompt = f""" Your job is to update the task based on the current state of the project.
â‹®----
messages_for_llm = [{"role": "user", "content": prompt}]
response = client.chat.completions.create(
â‹®----
messages=messages_for_llm, # Corrected variable name
max_tokens=get_constant("MAX_SUMMARY_TOKENS", 20000) # Use get_constant
â‹®----
new_task = response.choices[0].message.content
â‹®----
combined_content = f"""Original request:
````

## File: file_logger.py
````python
logger = logging.getLogger(__name__)
â‹®----
# Import the function but don't redefine it
â‹®----
# Define our own if not available in config
def convert_to_docker_path(path: Union[str, Path]) -> str
â‹®----
"""
            Convert a local Windows path to a Docker container path.
            No longer converts to Docker path, returns original path.
            Args:
                path: The local path to convert

            Returns:
                The original path as a string
            """
â‹®----
# Fallback if config module is not available
def get_constant(name)
â‹®----
# Default values for essential constants
defaults = {
â‹®----
def convert_to_docker_path(path: Union[str, Path]) -> str
â‹®----
"""
        Convert a local Windows path to a Docker container path.
        No longer converts to Docker path, returns original path.
        Args:
            path: The local path to convert

        Returns:
            The original path as a string
        """
â‹®----
# File for logging operations
â‹®----
LOG_FILE = get_constant("LOG_FILE")
â‹®----
LOG_FILE = os.path.join(
â‹®----
# In-memory tracking of file operations # FILE_OPERATIONS removed
# FILE_OPERATIONS = {} # Removed
â‹®----
# Ensure log directory exists
â‹®----
# If log file doesn't exist, create an empty one
â‹®----
# Track file operations # Removed unused global variables
# file_operations = [] # Removed
# tracked_files = set() # Removed
# file_contents = {} # Removed
â‹®----
"""
    Log a file operation (create, update, delete) with enhanced metadata handling.

    Args:
        file_path: Path to the file
        operation: Type of operation ('create', 'update', 'delete')
        content: Optional content for the file
        metadata: Optional dictionary containing additional metadata (e.g., image generation prompt)
    """
# Defensively initialize metadata to prevent NoneType errors
â‹®----
metadata = {}
â‹®----
# Ensure file_path is a Path object
â‹®----
file_path = Path(file_path)
â‹®----
# Create a string representation of the file path for consistent logging
file_path_str = str(file_path)
â‹®----
timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
extension = file_path.suffix.lower() if file_path.suffix else ""
â‹®----
# Determine if the file is an image
is_image = extension in [".png", ".jpg", ".jpeg", ".gif", ".webp", ".svg", ".bmp"]
mime_type = mimetypes.guess_type(file_path_str)[0]
â‹®----
is_image = True
â‹®----
# Track file operations in memory # Removed FILE_OPERATIONS update logic
# if file_path_str not in FILE_OPERATIONS:
#     FILE_OPERATIONS[file_path_str] = {
#         "operations": [],
#         "last_updated": timestamp,
#         "extension": extension,
#         "is_image": is_image,
#         "mime_type": mime_type,
#     }
#
# # Update the in-memory tracking
# FILE_OPERATIONS[file_path_str]["operations"].append(
#     {"timestamp": timestamp, "operation": operation}
# )
# FILE_OPERATIONS[file_path_str]["last_updated"] = timestamp
â‹®----
# Load existing log data or create a new one
log_data = {"files": {}}
â‹®----
log_data = json.load(f)
â‹®----
# If the log file is corrupted, start fresh
â‹®----
# Create or update the file entry in the log
â‹®----
# Add the operation to the log
â‹®----
# Update the metadata if provided
â‹®----
# Ensure we have a metadata dictionary
â‹®----
# Update with new metadata
â‹®----
# Store the content if provided, otherwise try to read it from the file
file_content = content
â‹®----
# Only try to read the file if it exists and content wasn't provided
â‹®----
# Handle different file types appropriately
â‹®----
# For images, store base64 encoded content
â‹®----
img_content = f.read()
â‹®----
# Add file size to metadata
â‹®----
# For code and text files, store as text
â‹®----
text_content = f.read()
â‹®----
# For other binary files, store base64 encoded
â‹®----
bin_content = f.read()
â‹®----
# Don't fail the entire operation, just log the error
â‹®----
# Use the provided content
â‹®----
# Don't fail the entire operation, just log the error
â‹®----
# Update last_updated timestamp
â‹®----
# Write the updated log data back to the log file
â‹®----
def aggregate_file_states() -> str
â‹®----
"""
    Collect information about all tracked files and their current state.

    Returns:
        A formatted string with information about all files.
    """
LOG_FILE = Path(get_constant("LOG_FILE"))
â‹®----
log_data = json.loads(f.read())
â‹®----
# Group files by type
image_files = []
code_files = []
text_files = []
other_files = []
â‹®----
file_type = file_info.get("file_type", "other")
â‹®----
# Get the Docker path for display
docker_path = file_info.get("docker_path", convert_to_docker_path(file_path))
â‹®----
# Sort operations by timestamp to get the latest state
operations = sorted(
â‹®----
latest_operation = operations[0] if operations else {"operation": "unknown"}
â‹®----
image_metadata = file_info.get("image_metadata", {})
â‹®----
basic_info = file_info.get("basic_info", {})
â‹®----
# Format the output
output = []
â‹®----
# Add syntax highlighting based on file extension
extension = Path(code["path"]).suffix.lower()
lang = get_language_from_extension(extension)
â‹®----
extension = Path(text["path"]).suffix.lower()
â‹®----
def extract_code_skeleton(source_code: Union[str, Path]) -> str
â‹®----
"""
    Extract a code skeleton from existing Python code.

    This function takes Python code and returns just the structure: imports,
    class definitions, method/function signatures, and docstrings, with
    implementations replaced by 'pass' statements.

    Args:
        source_code: Either a path to a Python file or a string containing Python code

    Returns:
        str: The extracted code skeleton
    """
# Load the source code
â‹®----
code_str = file.read()
â‹®----
code_str = str(source_code)
â‹®----
# Parse the code into an AST
â‹®----
tree = ast.parse(code_str)
â‹®----
# Extract imports
imports = []
â‹®----
module = node.module or ""
names = ", ".join(
â‹®----
# Helper function to handle complex attributes
def format_attribute(node)
â‹®----
"""Helper function to recursively format attribute expressions"""
â‹®----
# Add support for ast.Subscript nodes (like List[int])
â‹®----
# Use ast.unparse for Python 3.9+ or manual handling for earlier versions
â‹®----
# Simplified handling for common cases
â‹®----
base = node.value.id
â‹®----
base = format_attribute(node.value)
# Simple handling for slice
â‹®----
return f"{base}[...]"  # Fallback for complex slices
return f"{base}[...]"  # Fallback for complex cases
â‹®----
# Fallback for other node types - use ast.unparse if available
â‹®----
# Get docstrings and function/class signatures
class CodeSkeletonVisitor(ast.NodeVisitor)
â‹®----
def __init__(self)
â‹®----
def visit_Import(self, node)
â‹®----
# Already handled above
â‹®----
def visit_ImportFrom(self, node)
â‹®----
def visit_ClassDef(self, node)
â‹®----
# Extract class definition with inheritance
bases = []
â‹®----
# Use the helper function to handle nested attributes
â‹®----
# Fallback for other complex cases
â‹®----
class_def = f"class {node.name}"
â‹®----
# Add class definition
â‹®----
# Add docstring if it exists
docstring = ast.get_docstring(node)
â‹®----
doc_lines = docstring.split("\n")
â‹®----
# Increment indent for class members
â‹®----
# Visit all class members
â‹®----
# If no members were added, add a pass statement
â‹®----
# Restore indent
â‹®----
def visit_FunctionDef(self, node)
â‹®----
# Extract function signature
args = []
defaults = [None] * (
â‹®----
# Process regular arguments
â‹®----
arg_str = arg.arg
# Add type annotation if available
â‹®----
# Use the helper function to handle complex types
â‹®----
arg_str += ": ..."  # Fallback for complex annotations
â‹®----
# Add default value if available
â‹®----
# Simplified handling for common default values
â‹®----
arg_str += " = ..."  # Fallback for complex defaults
â‹®----
# Handle *args
â‹®----
# Handle keyword-only args
â‹®----
kw_str = kwarg.arg
â‹®----
kw_str += " = ..."  # Fallback for complex defaults
â‹®----
# Handle **kwargs
â‹®----
# Build function signature
func_def = f"def {node.name}({', '.join(args)})"
â‹®----
# Add return type if specified
â‹®----
# Add function definition
â‹®----
# Add pass statement in place of the function body
â‹®----
# Run the visitor on the AST
visitor = CodeSkeletonVisitor()
â‹®----
# Combine imports and code skeleton
result = []
â‹®----
# Add all imports first
â‹®----
result.append("")  # Add a blank line after imports
â‹®----
# Add the rest of the code skeleton
â‹®----
def get_all_current_code() -> str
â‹®----
"""
    Returns all the current code in the project as a string.
    This function is used to provide context about the existing code to the LLM.

    Returns:
        A string with all the current code.
    """
â‹®----
# Ensure log file exists
â‹®----
# Initialize a new log file so future operations work
â‹®----
# Load log data with robust error handling
â‹®----
# Reset the log file with valid JSON
â‹®----
# Validate log structure
â‹®----
# Fix the format
â‹®----
# Process each file in the log
â‹®----
# Skip files that have been deleted (last operation is 'delete')
operations = file_data.get("operations", [])
â‹®----
# Get content and metadata
content = file_data.get("content")
â‹®----
# Skip images and binary files
is_image = file_data.get("is_image", False)
mime_type = file_data.get("mime_type", "")
extension = file_data.get("extension", "").lower()
â‹®----
# Only include code files
â‹®----
# Continue processing other files
â‹®----
# Sort files by path for consistent output
â‹®----
# Format the output
â‹®----
lang = get_language_from_extension(code["extension"])
â‹®----
logger.error( # Replaced print with logger.error
â‹®----
# Add a simpler version without failing
â‹®----
# Return the formatted output (This was missing)
â‹®----
def get_all_current_skeleton() -> str
â‹®----
"""
    Get the skeleton of all Python code files.

    Returns:
        A formatted string with the skeleton of all Python code files.
    """
â‹®----
output = ["# All Python File Skeletons"]
â‹®----
# Skip files that have been deleted (last operation is 'delete')
operations = file_info.get("operations", [])
â‹®----
# Only process Python files
extension = file_info.get("extension", "").lower()
â‹®----
# Get Docker path for display
docker_path = convert_to_docker_path(file_path)
â‹®----
# Look for skeleton in metadata
metadata = file_info.get("metadata", {})
skeleton = metadata.get("skeleton", "")
â‹®----
# If no skeleton in metadata, try to extract it from the content
â‹®----
skeleton = extract_code_skeleton(file_info.get("content", ""))
â‹®----
skeleton = "# Failed to extract skeleton"
â‹®----
# Add file header
â‹®----
# Add skeleton with syntax highlighting
â‹®----
def get_language_from_extension(extension: str) -> str
â‹®----
"""
    Map file extensions to programming languages for syntax highlighting.

    Args:
        extension: The file extension (e.g., '.py', '.js')

    Returns:
        The corresponding language name for syntax highlighting.
    """
extension = extension.lower()
mapping = {
â‹®----
def should_skip_for_zip(path)
â‹®----
"""
    Determine if a file or directory should be skipped when creating a ZIP file.

    Args:
        path: Path to check

    Returns:
        bool: True if the path should be skipped, False otherwise
    """
path_str = str(path).lower()
â‹®----
# Skip virtual environment files and directories
â‹®----
# On Windows, particularly skip Linux-style virtual env paths
â‹®----
# Skip common directories not needed in the ZIP
dirs_to_skip = [
â‹®----
def archive_logs()
â‹®----
"""Archive all log files in LOGS_DIR by moving them to an archive folder with a timestamp."""
â‹®----
# Create timestamp for the archive folder
timestamp = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
archive_dir = Path(LOGS_DIR, "archive", timestamp)
â‹®----
# Get all files in LOGS_DIR
log_path = Path(LOGS_DIR)
log_files = [f for f in log_path.iterdir() if f.is_file()]
â‹®----
# Skip archiving if there are no files
â‹®----
# Move each file to the archive directory
â‹®----
# Skip archive directory itself
â‹®----
# Create destination path
dest_path = Path(archive_dir, file_path.name)
â‹®----
# Copy the file if it exists (some might be created later)
â‹®----
# Clear the original file but keep it
````

## File: llm_client.py
````python
logger = logging.getLogger(__name__)
â‹®----
class LLMClient(ABC)
â‹®----
"""Abstract base class for LLM clients."""
â‹®----
@abstractmethod
    async def call(self, messages: List[Dict[str, str]], max_tokens: int = 200, temperature: float = 0.1) -> str
â‹®----
"""Call the LLM with messages and return response."""
â‹®----
class OpenRouterClient(LLMClient)
â‹®----
"""OpenRouter API client."""
â‹®----
def __init__(self, model: str)
â‹®----
async def call(self, messages: List[Dict[str, str]], max_tokens: int = 200, temperature: float = 0.1) -> str
â‹®----
headers = {
â‹®----
payload = {
â‹®----
error_text = await response.text()
â‹®----
result = await response.json()
â‹®----
class OpenAIClient(LLMClient)
â‹®----
"""OpenAI API client."""
â‹®----
class AnthropicClient(LLMClient)
â‹®----
"""Anthropic API client."""
â‹®----
# Convert messages format for Anthropic
system_content = ""
user_messages = []
â‹®----
system_content = msg["content"]
â‹®----
def create_llm_client(model: str) -> LLMClient
â‹®----
"""Factory function to create appropriate LLM client based on model name."""
â‹®----
# Default to OpenRouter for unknown models
````

## File: miniOR.py
````python
openai41mini : str = "openai/gpt-4.1-mini"
gemma3n4b : str = "google/gemma-3n-e4b-it"
sonnet4 : str = "anthropic/claude-sonnet-4"
openai41 : str = "openai/gpt-4.1"
openaio3 : str = "openai/o3"
openaio3pro : str = "openai/o3-pro"
googlepro : str = "google/gemini-2.5-pro-preview"
googleflash : str = "google/gemini-2.5-flash-preview"
googleflashlite : str = "google/gemini-2.5-flash-lite-preview-06-17"
grok4 : str = "x-ai/grok-4"
SUMMARY_MODEL : str = googleflashlite  # Model for summaries
MAIN_MODEL : str = f"{googleflashlite}"  # Primary model for main agent operations
CODE_MODEL : str = f"{googleflashlite}:web"  # Model for code generation tasks
BASE_URL : str = "https://openrouter.ai/api/v1"
DEFAULT_MODEL : str = MAIN_MODEL
â‹®----
BASE_SYSTEM_PROMPT : str = "You are a helpful AI assistant. "
â‹®----
"""
    Get the appropriate LLM instance based on the async flag.
    """
â‹®----
def chat(prompt_str, model=os.getenv("OPENROUTER_MODEL", DEFAULT_MODEL))
â‹®----
"""
    Send a chat message to the LLM.
    
    """
llm = get_llm()
messages = [{"role": "user", "content": prompt_str}]
â‹®----
# Example usage
response = chat(prompt_str="Hello, how can you assist me today?")
````

## File: output_manager.py
````python
from config import get_constant  # Updated import
â‹®----
logger = logging.getLogger(__name__)
â‹®----
class OutputManager
â‹®----
LOGS_DIR = Path(get_constant("LOGS_DIR"))
â‹®----
def save_image(self, base64_data: str) -> Optional[Path]
â‹®----
"""Save base64 image data to file and return path."""
â‹®----
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
image_hash = hashlib.md5(base64_data.encode()).hexdigest()[:8]
image_path = self.image_dir / f"image_{timestamp}_{image_hash}.png"
â‹®----
image_data = base64.b64decode(base64_data)
â‹®----
def format_tool_output(self, result: "ToolResult", tool_name: str)
â‹®----
"""Format and display tool output."""
â‹®----
output_text = f"Used Tool: {tool_name}\n"
â‹®----
text = self._truncate_string(
â‹®----
image_path = self.save_image(result.base64_image)
â‹®----
# self.display., output_text)
â‹®----
def format_api_response(self, response: Dict[str, Any])
â‹®----
"""Format and display API response."""
â‹®----
def format_content_block(self, block: Dict[str, Any]) -> None
â‹®----
"""Format and display content block."""
â‹®----
safe_input = {
â‹®----
"""Format and display recent conversation."""
â‹®----
# recent_messages = messages[:num_recent] if len(messages) > num_recent else messages
recent_messages = messages[-num_recent:]
â‹®----
def _format_user_content(self, content: Any)
â‹®----
"""Format and display user content."""
â‹®----
#     self.display., text)
# elif item.get("type") == "image":
#     self.display., "ğŸ“¸ Screenshot captured")
â‹®----
# self.display., text)
â‹®----
def _format_assistant_content(self, content: Any)
â‹®----
"""Format and display assistant content."""
â‹®----
tool_input = content_block.get("input", "")
â‹®----
tool_input = json.loads(tool_input)
â‹®----
# self.display., (tool_name, f"Input: {input_text}"))
â‹®----
def _truncate_string(self, text: str, max_length: int = 500) -> str
â‹®----
"""Truncate a string to a max length with ellipsis."""
````

## File: web_ui.py
````python
def log_message(msg_type, message)
â‹®----
"""Log a message to a file."""
â‹®----
emojitag = "ğŸ¤¡ "
â‹®----
emojitag = "ğŸ§â€â™€ï¸ "
â‹®----
emojitag = "ğŸ“ "
â‹®----
emojitag = "â“ "
log_file = os.path.join(LOGS_DIR, f"{msg_type}_messages.log")
â‹®----
class WebUI
â‹®----
def __init__(self, agent_runner)
â‹®----
# More robust path for templates
template_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', 'templates'))
â‹®----
# Using a standard Queue for cross-thread communication
â‹®----
# Import tools lazily to avoid circular imports
â‹®----
# BashTool,
# OpenInterpreterTool,
â‹®----
# OpenInterpreterTool(display=self),  # Uncommented and enabled for testing
â‹®----
def setup_routes(self)
â‹®----
@self.app.route("/")
        def select_prompt_route()
â‹®----
prompt_files = list(PROMPTS_DIR.glob("*.md"))
options = [file.name for file in prompt_files]
â‹®----
@self.app.route("/classic")
        def select_prompt_classic_route()
â‹®----
@self.app.route("/modern")
        def select_prompt_modern_route()
â‹®----
@self.app.route("/run_agent", methods=["POST"])
        def run_agent_route()
â‹®----
choice = request.form.get("choice")
filename = request.form.get("filename")
prompt_text = request.form.get("prompt_text")
â‹®----
new_prompt_path = PROMPTS_DIR / f"{filename}.md"
prompt_name = Path(filename).stem
â‹®----
task = prompt_text or ""
â‹®----
prompt_path = PROMPTS_DIR / choice
â‹®----
prompt_name = prompt_path.stem
â‹®----
task = f.read()
filename = prompt_path.stem
â‹®----
# Configure repository directory for this prompt
base_repo_dir = Path(get_constant("TOP_LEVEL_DIR")) / "repo"
repo_dir = base_repo_dir / prompt_name
â‹®----
coro = self.agent_runner(task, self)
â‹®----
@self.app.route("/messages")
        def get_messages()
â‹®----
@self.app.route("/api/prompts/<path:filename>")
        def api_get_prompt(filename)
â‹®----
"""Return the raw content of a prompt file."""
â‹®----
prompt_path = PROMPTS_DIR / filename
â‹®----
data = f.read()
â‹®----
@self.app.route("/api/tasks")
        def api_get_tasks()
â‹®----
"""Return the list of available tasks."""
â‹®----
tasks = [file.name for file in prompt_files]
â‹®----
@self.app.route("/api/files")
        def api_get_files()
â‹®----
"""Return the file tree."""
repo_dir = get_constant("REPO_DIR")
â‹®----
def get_file_tree(path)
â‹®----
tree = []
â‹®----
node = {"name": item.name, "path": str(item.relative_to(repo_dir))}
â‹®----
@self.app.route("/api/file_tree")
        def api_file_list()
â‹®----
"""Return a list of files under the current repository."""
repo_dir = Path(get_constant("REPO_DIR"))
files = [
â‹®----
@self.app.route("/api/file")
        def api_get_file()
â‹®----
"""Return the contents of a file within the repo."""
rel_path = request.args.get("path", "")
â‹®----
safe_path = os.path.normpath(rel_path)
â‹®----
file_path = repo_dir / safe_path
â‹®----
content = f.read()
â‹®----
@self.app.route("/api/files/content")
        def api_get_file_content()
â‹®----
"""Return the content of a file."""
â‹®----
file_path = request.args.get("path")
â‹®----
abs_path = Path(repo_dir) / file_path
â‹®----
@self.app.route("/tools")
        def tools_route()
â‹®----
"""Display available tools."""
tool_list = []
â‹®----
info = tool.to_params()["function"]
â‹®----
@self.app.route("/tools/<tool_name>", methods=["GET", "POST"])
        def run_tool_route(tool_name)
â‹®----
"""Run an individual tool from the toolbox."""
tool = self.tool_collection.tools.get(tool_name)
â‹®----
params = tool.to_params()["function"]["parameters"]
result_text = None
â‹®----
tool_input = {}
â‹®----
value = request.form.get(param)
â‹®----
pinfo = params["properties"].get(param, {})
â‹®----
result = asyncio.run(self.tool_collection.run(tool_name, tool_input))
result_text = result.output or result.error
â‹®----
result_text = str(exc)
â‹®----
@self.app.route("/browser")
        def file_browser_route()
â‹®----
"""Serve the VS Code-style file browser interface."""
â‹®----
@self.app.route("/api/file-tree")
        def api_file_tree()
â‹®----
"""Return the file tree structure for the current REPO_DIR."""
â‹®----
def build_tree(path)
â‹®----
items = []
â‹®----
# Skip hidden files and directories
â‹®----
tree = build_tree(repo_dir)
â‹®----
@self.app.route("/api/file-content")
        def api_file_content()
â‹®----
"""Return the content of a specific file."""
file_path = request.args.get('path')
â‹®----
path = Path(file_path)
# Security check - ensure the path is within REPO_DIR
â‹®----
# Try to read as text, handle binary files gracefully
â‹®----
# If it's a binary file, return a message instead
â‹®----
def setup_socketio_events(self)
â‹®----
@self.socketio.on("connect")
        def handle_connect()
â‹®----
@self.socketio.on("disconnect")
        def handle_disconnect()
â‹®----
@self.socketio.on("user_input")
        def handle_user_input(data)
â‹®----
user_input = data.get("message", "") or data.get("input", "")
â‹®----
# Queue is thread-safe; use blocking put to notify waiting tasks
â‹®----
@self.socketio.on("tool_response")
        def handle_tool_response(data)
â‹®----
params = data.get("input", {}) if data.get("action") != "cancel" else {}
â‹®----
@self.socketio.on("interrupt_agent")
        def handle_interrupt_agent()
â‹®----
# This could be used to signal the agent to stop processing
â‹®----
def start_server(self, host="0.0.0.0", port=5002)
â‹®----
def add_message(self, msg_type, content)
â‹®----
# Also emit to file browser
â‹®----
# Parse tool result for file browser
â‹®----
lines = content.split('\n')
tool_name = "Unknown"
â‹®----
first_line = lines[0].strip()
â‹®----
tool_name = first_line.replace('Tool:', '').strip()
â‹®----
# Check if this tool might have created/modified files
â‹®----
# Emit file tree update after a short delay asynchronously
â‹®----
def broadcast_update(self)
â‹®----
async def wait_for_user_input(self, prompt_message: str | None = None) -> str
â‹®----
"""Await the next user input sent via the web UI input queue."""
â‹®----
loop = asyncio.get_running_loop()
user_response = await loop.run_in_executor(None, self.input_queue.get)
â‹®----
# Clear the prompt after input is received
â‹®----
async def confirm_tool_call(self, tool_name: str, args: dict, schema: dict) -> dict | None
â‹®----
"""Send a tool prompt to the web UI and wait for edited parameters."""
â‹®----
params = await loop.run_in_executor(None, self.tool_queue.get)
````
