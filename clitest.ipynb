{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e253360e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Dict, Optional, Any, Union\n",
    "\n",
    "# Ensure you have the necessary libraries installed:\n",
    "# pip install openai anthropic python-dotenv\n",
    "# Create a .env file in your project root for API keys if you prefer:\n",
    "# OPENAI_API_KEY=\"your_openai_key\"\n",
    "# ANTHROPIC_API_KEY=\"your_anthropic_key\"\n",
    "# OPENROUTER_API_KEY=\"your_openrouter_key\"\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # Load environment variables from .env file\n",
    "\n",
    "\n",
    "class MinimalLLMClient:\n",
    "    \"\"\"\n",
    "    A minimal LLM client for OpenAI, Anthropic, and OpenAI-compatible (OpenRouter) services,\n",
    "    with model and default parameters defined at initialization.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        provider: str,\n",
    "        model: str,\n",
    "        api_key: Optional[str] = None,\n",
    "        base_url: Optional[str] = None,\n",
    "        temperature: float = 0.7,\n",
    "        max_tokens: int = 1024,\n",
    "        system_prompt: Optional[str] = \"You are a helpful assistant.\",\n",
    "        openrouter_site_url: Optional[str] = None,\n",
    "        openrouter_app_name: Optional[str] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the LLM client.\n",
    "\n",
    "        Args:\n",
    "            provider (str): The LLM provider. Supported: \"openai\", \"anthropic\", \"openrouter\".\n",
    "            model (str): The default model name to use for this client instance.\n",
    "            api_key (Optional[str]): The API key for the provider.\n",
    "                                     If None, it will try to read from environment variables:\n",
    "                                     - OPENAI_API_KEY for \"openai\"\n",
    "                                     - ANTHROPIC_API_KEY for \"anthropic\"\n",
    "                                     - OPENROUTER_API_KEY for \"openrouter\" (falls back to OPENAI_API_KEY)\n",
    "            base_url (Optional[str]): The base URL for OpenAI-compatible services.\n",
    "                                      Required for \"openrouter\" if not using the default.\n",
    "                                      Default for \"openrouter\": \"https://openrouter.ai/api/v1\".\n",
    "                                      Ignored for \"openai\" and \"anthropic\" official APIs.\n",
    "            temperature (float): Default sampling temperature.\n",
    "            max_tokens (int): Default maximum number of tokens to generate.\n",
    "            system_prompt (Optional[str]): Default system prompt.\n",
    "            openrouter_site_url (Optional[str]): Your site URL, for OpenRouter's HTTP-Referer header.\n",
    "            openrouter_app_name (Optional[str]): Your app name, for OpenRouter's X-Title header.\n",
    "        \"\"\"\n",
    "        self.provider = provider.lower()\n",
    "        self.model = model\n",
    "        self.temperature = temperature\n",
    "        self.max_tokens = max_tokens\n",
    "        self.system_prompt = system_prompt\n",
    "        self.openrouter_site_url = openrouter_site_url\n",
    "        self.openrouter_app_name = openrouter_app_name\n",
    "\n",
    "        if api_key is None:\n",
    "            if self.provider == \"openai\":\n",
    "                api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "            elif self.provider == \"anthropic\":\n",
    "                api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "            elif self.provider == \"openrouter\":\n",
    "                api_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "                if not api_key:\n",
    "                    api_key = os.getenv(\"OPENAI_API_KEY\")  # Fallback for OpenRouter\n",
    "\n",
    "        if not api_key:\n",
    "            raise ValueError(\n",
    "                f\"API key for {provider} not provided or found in environment variables.\"\n",
    "            )\n",
    "        self.api_key = api_key\n",
    "\n",
    "        if self.provider == \"openai\":\n",
    "            from openai import OpenAI as OpenAIClient\n",
    "\n",
    "            self.client = OpenAIClient(api_key=self.api_key)\n",
    "        elif self.provider == \"anthropic\":\n",
    "            from anthropic import Anthropic as AnthropicClient\n",
    "\n",
    "            self.client = AnthropicClient(api_key=self.api_key)\n",
    "        elif self.provider == \"openrouter\":\n",
    "            from openai import OpenAI as OpenAIClient  # OpenRouter uses OpenAI SDK\n",
    "\n",
    "            if base_url is None:\n",
    "                base_url = \"https://openrouter.ai/api/v1\"\n",
    "            self.base_url = base_url\n",
    "            self.client = OpenAIClient(api_key=self.api_key, base_url=self.base_url)\n",
    "            self.extra_headers = {}\n",
    "            if self.openrouter_site_url:\n",
    "                self.extra_headers[\"HTTP-Referer\"] = self.openrouter_site_url\n",
    "            if self.openrouter_app_name:\n",
    "                self.extra_headers[\"X-Title\"] = self.openrouter_app_name\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Unsupported provider: {provider}. Supported providers are 'openai', 'anthropic', 'openrouter'.\"\n",
    "            )\n",
    "\n",
    "    def set_model(self, model: str) -> \"MinimalLLMClient\":\n",
    "        \"\"\"Sets the default model for this client.\"\"\"\n",
    "        self.model = model\n",
    "        return self\n",
    "\n",
    "    def set_system_prompt(self, system_prompt: Optional[str]) -> \"MinimalLLMClient\":\n",
    "        \"\"\"Sets the default system prompt for this client.\"\"\"\n",
    "        self.system_prompt = system_prompt\n",
    "        return self\n",
    "\n",
    "    def set_max_tokens(self, max_tokens: int) -> \"MinimalLLMClient\":\n",
    "        \"\"\"Sets the default maximum tokens for this client.\"\"\"\n",
    "        if max_tokens <= 0:\n",
    "            raise ValueError(\"max_tokens must be a positive integer.\")\n",
    "        self.max_tokens = max_tokens\n",
    "        return self\n",
    "\n",
    "    def set_temperature(self, temperature: float) -> \"MinimalLLMClient\":\n",
    "        \"\"\"Sets the default temperature for this client.\"\"\"\n",
    "        if not (0.0 <= temperature <= 2.0):  # OpenAI typical range\n",
    "            # Anthropic range is 0.0 to 1.0, but we'll use OpenAI's broader range for consistency here\n",
    "            print(\n",
    "                f\"Warning: Temperature {temperature} might be outside the typical optimal range for some models.\"\n",
    "            )\n",
    "        self.temperature = temperature\n",
    "        return self\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        user_prompt: str,\n",
    "        temperature: Optional[float] = None,\n",
    "        max_tokens: Optional[int] = None,\n",
    "        system_prompt: Optional[str] = None,\n",
    "        model_override: Optional[str] = None,\n",
    "        messages_override: Optional[List[Dict[str, str]]] = None,\n",
    "    ) -> Any:  # Returns the raw response object from the SDK\n",
    "        \"\"\"\n",
    "        Calls the LLM with the given user prompt and other optional parameters.\n",
    "\n",
    "        Args:\n",
    "            user_prompt (str): The user's prompt.\n",
    "            temperature (Optional[float]): Override instance's default temperature.\n",
    "            max_tokens (Optional[int]): Override instance's default max_tokens.\n",
    "            system_prompt (Optional[str]): Override instance's default system_prompt.\n",
    "                                           Set to empty string \"\" to explicitly have no system prompt\n",
    "                                           if instance default is set.\n",
    "            model_override (Optional[str]): Override the instance's default model for this call.\n",
    "            messages_override (Optional[List[Dict[str, str]]]):\n",
    "                If provided, this list of messages will be used directly,\n",
    "                ignoring user_prompt and system_prompt arguments.\n",
    "\n",
    "        Returns:\n",
    "            Any: The raw response object from the underlying SDK (e.g., OpenAI's ChatCompletion, Anthropic's Message).\n",
    "\n",
    "        Raises:\n",
    "            Exception: If the API call fails.\n",
    "        \"\"\"\n",
    "        current_model = model_override if model_override is not None else self.model\n",
    "        current_temp = temperature if temperature is not None else self.temperature\n",
    "        current_max_tokens = max_tokens if max_tokens is not None else self.max_tokens\n",
    "\n",
    "        # Handle system_prompt override: if explicitly passed as None, use self.system_prompt.\n",
    "        # If passed as a string (even empty \"\"), use that.\n",
    "        current_system_prompt = (\n",
    "            system_prompt if system_prompt is not None else self.system_prompt\n",
    "        )\n",
    "\n",
    "        actual_messages: List[Dict[str, str]]\n",
    "        if messages_override:\n",
    "            actual_messages = messages_override\n",
    "        else:\n",
    "            actual_messages = []\n",
    "            if (\n",
    "                current_system_prompt and self.provider != \"anthropic\"\n",
    "            ):  # Anthropic handles system via a dedicated param\n",
    "                actual_messages.append(\n",
    "                    {\"role\": \"system\", \"content\": current_system_prompt}\n",
    "                )\n",
    "            actual_messages.append({\"role\": \"user\", \"content\": user_prompt})\n",
    "\n",
    "        try:\n",
    "            if self.provider == \"openai\" or self.provider == \"openrouter\":\n",
    "                api_params = {\n",
    "                    \"model\": current_model,\n",
    "                    \"messages\": actual_messages,\n",
    "                    \"temperature\": current_temp,\n",
    "                    \"max_tokens\": current_max_tokens,\n",
    "                }\n",
    "                if self.provider == \"openrouter\" and self.extra_headers:\n",
    "                    api_params[\"extra_headers\"] = self.extra_headers\n",
    "\n",
    "                # For OpenAI, if system prompt is handled outside messages_override and\n",
    "                # was intended as the first message in actual_messages.\n",
    "                # If messages_override is used, it should contain the system prompt if needed.\n",
    "                # If not using messages_override, and system prompt is set, and provider is openai/openrouter\n",
    "                # ensure it's part of actual_messages (which it is by default construction above)\n",
    "\n",
    "                return self.client.chat.completions.create(**api_params)\n",
    "\n",
    "            elif self.provider == \"anthropic\":\n",
    "                # Anthropic's Messages API uses a top-level `system` parameter.\n",
    "                # `actual_messages` here should not contain the system message if it was built from user_prompt.\n",
    "                anthropic_messages = [\n",
    "                    msg for msg in actual_messages if msg.get(\"role\") != \"system\"\n",
    "                ]\n",
    "\n",
    "                # If messages_override was used, extract system prompt from it if present.\n",
    "                final_system_prompt_for_anthropic = current_system_prompt\n",
    "                if messages_override:\n",
    "                    for msg in messages_override:\n",
    "                        if msg.get(\"role\") == \"system\":\n",
    "                            final_system_prompt_for_anthropic = msg.get(\"content\")\n",
    "                            anthropic_messages = [\n",
    "                                m\n",
    "                                for m in messages_override\n",
    "                                if m.get(\"role\") != \"system\"\n",
    "                            ]\n",
    "                            break\n",
    "\n",
    "                api_params = {\n",
    "                    \"model\": current_model,\n",
    "                    \"messages\": anthropic_messages,\n",
    "                    \"temperature\": current_temp,\n",
    "                    \"max_tokens\": current_max_tokens,\n",
    "                }\n",
    "                if (\n",
    "                    final_system_prompt_for_anthropic\n",
    "                ):  # Only add if it's not None or empty\n",
    "                    api_params[\"system\"] = final_system_prompt_for_anthropic\n",
    "\n",
    "                return self.client.messages.create(**api_params)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                f\"Error calling LLM provider {self.provider} with model {current_model}: {e}\"\n",
    "            )\n",
    "            raise\n",
    "\n",
    "    def get_text(self, response: Any) -> str:\n",
    "        \"\"\"\n",
    "        Extracts the primary text content from the LLM's response object.\n",
    "\n",
    "        Args:\n",
    "            response (Any): The raw response object from the generate() method.\n",
    "\n",
    "        Returns:\n",
    "            str: The generated text content.\n",
    "        \"\"\"\n",
    "        if self.provider == \"openai\" or self.provider == \"openrouter\":\n",
    "            if response and response.choices and len(response.choices) > 0:\n",
    "                message = response.choices[0].message\n",
    "                if message and message.content:\n",
    "                    return message.content.strip()\n",
    "            return \"\"\n",
    "        elif self.provider == \"anthropic\":\n",
    "            if response and response.content:\n",
    "                generated_text = []\n",
    "                for block in response.content:\n",
    "                    if block.type == \"text\":\n",
    "                        generated_text.append(block.text)\n",
    "                return \"\".join(generated_text).strip()\n",
    "            return \"\"\n",
    "        return \"Unsupported provider for get_text or invalid response.\"\n",
    "\n",
    "    def get_meta(self, response: Any) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Extracts metadata from the LLM's response object into a standardized dictionary.\n",
    "\n",
    "        Args:\n",
    "            response (Any): The raw response object from the generate() method.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, Any]: A dictionary containing metadata.\n",
    "        \"\"\"\n",
    "        meta: Dict[str, Any] = {\n",
    "            \"model_used\": None,\n",
    "            \"input_tokens\": None,\n",
    "            \"output_tokens\": None,\n",
    "            \"total_tokens\": None,\n",
    "            \"finish_reason\": None,\n",
    "            \"raw_response_id\": None,\n",
    "            \"provider_specific\": {},\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            if self.provider == \"openai\" or self.provider == \"openrouter\":\n",
    "                if response:\n",
    "                    meta[\"model_used\"] = response.model\n",
    "                    meta[\"raw_response_id\"] = response.id\n",
    "                    if response.usage:\n",
    "                        meta[\"input_tokens\"] = response.usage.prompt_tokens\n",
    "                        meta[\"output_tokens\"] = response.usage.completion_tokens\n",
    "                        meta[\"total_tokens\"] = response.usage.total_tokens\n",
    "                    if response.choices and len(response.choices) > 0:\n",
    "                        meta[\"finish_reason\"] = response.choices[0].finish_reason\n",
    "                    meta[\"provider_specific\"] = {\n",
    "                        \"system_fingerprint\": getattr(\n",
    "                            response, \"system_fingerprint\", None\n",
    "                        )\n",
    "                    }\n",
    "            elif self.provider == \"anthropic\":\n",
    "                if response:\n",
    "                    meta[\"model_used\"] = response.model\n",
    "                    meta[\"raw_response_id\"] = response.id\n",
    "                    meta[\"finish_reason\"] = response.stop_reason  # or stop_sequence\n",
    "                    if response.usage:\n",
    "                        meta[\"input_tokens\"] = response.usage.input_tokens\n",
    "                        meta[\"output_tokens\"] = response.usage.output_tokens\n",
    "                        meta[\"total_tokens\"] = (\n",
    "                            response.usage.input_tokens + response.usage.output_tokens\n",
    "                        )\n",
    "                    meta[\"provider_specific\"] = {\n",
    "                        \"role\": response.role,\n",
    "                        \"stop_sequence\": getattr(response, \"stop_sequence\", None),\n",
    "                    }\n",
    "        except AttributeError as e:\n",
    "            print(f\"Could not extract some metadata, attribute missing: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while extracting metadata: {e}\")\n",
    "\n",
    "        return meta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc15177",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- Example Usage ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"--- MinimalLLMClient: Refactored Examples ---\")\n",
    "\n",
    "    # --- OpenRouter Example (Model defined at init) ---\n",
    "    print(\"\\n--- OpenRouter (Google Gemini Flash) Example ---\")\n",
    "    try:\n",
    "        or_gem_client = MinimalLLMClient(\n",
    "            provider=\"openrouter\",\n",
    "            model=\"google/gemini-flash-1.5\",  # Model defined here\n",
    "            system_prompt=\"You are a concise and factual assistant.\",\n",
    "            openrouter_site_url=\"http://my-app.com\",  # Optional\n",
    "            openrouter_app_name=\"MyGreatLLMApp\",  # Optional\n",
    "            # API key will be loaded from .env or environment variables\n",
    "        )\n",
    "        response_obj = or_gem_client.generate(\n",
    "            \"Tell me three interesting facts about Mars.\"\n",
    "        )\n",
    "        text_response = or_gem_client.get_text(response_obj)\n",
    "        meta_response = or_gem_client.get_meta(response_obj)\n",
    "        print(f\"Text Response:\\n{text_response}\")\n",
    "        print(f\"Metadata:\\n{meta_response}\")\n",
    "\n",
    "        # Change system prompt and max_tokens for subsequent calls\n",
    "        or_gem_client.set_system_prompt(\n",
    "            \"You are a poet writing about planets.\"\n",
    "        ).set_max_tokens(50)\n",
    "        print(\n",
    "            f\"\\nClient's new system prompt: '{or_gem_client.system_prompt}', max_tokens: {or_gem_client.max_tokens}\"\n",
    "        )\n",
    "\n",
    "        response_obj_2 = or_gem_client.generate(\"A short verse on Venus.\")\n",
    "        print(f\"Poetic Response:\\n{or_gem_client.get_text(response_obj_2)}\")\n",
    "        print(f\"Metadata:\\n{or_gem_client.get_meta(response_obj_2)}\")\n",
    "\n",
    "        # Override parameters for a single call\n",
    "        print(\"\\nOverriding temperature and model for a single call:\")\n",
    "        response_obj_3 = or_gem_client.generate(\n",
    "            user_prompt=\"What is OpenRouter?\",\n",
    "            temperature=0.2,\n",
    "            model_override=\"mistralai/mistral-7b-instruct\",  # Using a different model via OpenRouter\n",
    "        )\n",
    "        print(f\"Text Response (Mistral):\\n{or_gem_client.get_text(response_obj_3)}\")\n",
    "        print(f\"Metadata (Mistral):\\n{or_gem_client.get_meta(response_obj_3)}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"OpenRouter Example Failed: {e}\")\n",
    "\n",
    "    # --- OpenAI Example ---\n",
    "    print(\"\\n--- OpenAI Example ---\")\n",
    "    try:\n",
    "        openai_gpt_client = MinimalLLMClient(\n",
    "            provider=\"openai\", model=\"gpt-4o-mini\", temperature=0.8, max_tokens=150\n",
    "        )\n",
    "        openai_gpt_client.set_system_prompt(\"You are an expert storyteller.\")\n",
    "        response_obj = openai_gpt_client.generate(\n",
    "            \"Tell a very short story about a brave mouse.\"\n",
    "        )\n",
    "        print(f\"Story:\\n{openai_gpt_client.get_text(response_obj)}\")\n",
    "        print(f\"Metadata:\\n{openai_gpt_client.get_meta(response_obj)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"OpenAI Example Failed: {e}\")\n",
    "\n",
    "    # --- Anthropic Example ---\n",
    "    print(\"\\n--- Anthropic Example ---\")\n",
    "    try:\n",
    "        anthropic_claude_client = MinimalLLMClient(\n",
    "            provider=\"anthropic\",\n",
    "            model=\"claude-3-haiku-20240307\",\n",
    "            system_prompt=\"You explain complex topics simply.\",\n",
    "        )\n",
    "        response_obj = anthropic_claude_client.generate(\n",
    "            user_prompt=\"What is the main idea behind quantum entanglement?\",\n",
    "            max_tokens=200,\n",
    "        )\n",
    "        print(f\"Explanation:\\n{anthropic_claude_client.get_text(response_obj)}\")\n",
    "        print(f\"Metadata:\\n{anthropic_claude_client.get_meta(response_obj)}\")\n",
    "\n",
    "        # Example using messages_override for a more complex interaction\n",
    "        print(\"\\nAnthropic example with messages_override:\")\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Hello Claude, can you write a haiku about seasons?\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"Green leaves softly fall,\\nWinter's chill then sun's warm kiss,\\nNature's gentle spin.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"That was lovely! Can you write another about the ocean?\",\n",
    "            },\n",
    "        ]\n",
    "        response_obj_msg_override = anthropic_claude_client.generate(\n",
    "            user_prompt=\"\",  # Ignored when messages_override is used\n",
    "            messages_override=messages,\n",
    "            system_prompt=\"You are a Haiku master.\",  # This will be used by Anthropic if not in messages_override\n",
    "        )\n",
    "        print(\n",
    "            f\"Ocean Haiku:\\n{anthropic_claude_client.get_text(response_obj_msg_override)}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Metadata:\\n{anthropic_claude_client.get_meta(response_obj_msg_override)}\"\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Anthropic Example Failed: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
