At the bottom is a detailed description of code that you need to write, followed by a code skeleton that provides the structure.
                    
        Active Task: # Expanded Task Description: Reinforcement Learning Agents for Battleship in Pygame

The task is to develop a system where two reinforcement learning (RL) agents play the game Battleship against each other. These agents must use different reinforcement learning methods to make decisions and improve over time. The project will implement:

1. A Battleship game engine using Pygame
2. Two distinct RL models:
   - Model A: Deep Q-Network (DQN) approach
   - Model B: Policy Gradient approach
3. A training pipeline where agents play against each other
4. A visualization system to display:
   - The actual game boards during play
   - Live gameplay every 100 matches
   - Training metrics and win/loss statistics

Ship placement will be random for both models at the beginning of each game. The game will follow standard Battleship rules with a 10x10 grid and the traditional set of ships (Carrier, Battleship, Cruiser, Submarine, Destroyer).

The Pygame window will be divided into two sections:
- Top section: Displays the game boards showing ships and hits/misses
- Bottom section: Shows training metrics such as reward curves, win rates, loss values, and a comparison between the two models

The system will train continuously, with every 100th match shown live on screen for human observation. No human gameplay interface is needed, nor are sound effects or extensive documentation.

# File Tree

1. **main.py**
   - Purpose: Entry point for the program, sets up the game loop and training process
   - Import: No import needed (this is the root file)

2. **game/battleship.py**
   - Purpose: Implements the Battleship game logic, board state, rules, and mechanics
   - Import: `from game.battleship import Battleship, Board`

3. **game/constants.py**
   - Purpose: Contains all game constants like board size, ship sizes, colors, screen dimensions
   - Import: `from game.constants import *`

4. **models/dqn_agent.py**
   - Purpose: Implements the Deep Q-Network agent (Model A)
   - Import: `from models.dqn_agent import DQNAgent`

5. **models/policy_gradient_agent.py**
   - Purpose: Implements the Policy Gradient agent (Model B)
   - Import: `from models.policy_gradient_agent import PolicyGradientAgent`

6. **models/replay_buffer.py**
   - Purpose: Implements experience replay buffer for the RL agents
   - Import: `from models.replay_buffer import ReplayBuffer`

7. **models/neural_nets.py**
   - Purpose: Contains neural network architectures used by both agents
   - Import: `from models.neural_nets import DQNNetwork, PolicyNetwork`

8. **visualization/renderer.py**
   - Purpose: Handles the Pygame rendering of game boards and ships
   - Import: `from visualization.renderer import GameRenderer`

9. **visualization/metrics.py**
   - Purpose: Manages and displays training metrics and statistics
   - Import: `from visualization.metrics import MetricsVisualizer`

10. **training/trainer.py**
    - Purpose: Manages the training process, episodes, and agent interactions
    - Import: `from training.trainer import BattleshipTrainer`

11. **assets/sprites.py**
    - Purpose: Loads and manages game sprites and visual assets
    - Import: `from assets.sprites import SpriteManager`

This structure keeps the project organized while minimizing the number of subdirectories and files, focusing on a clear separation of game logic, RL models, visualization, and training components.Start testing as soon as possible. DO NOT start making fixes or improvements until you have tested to see if it is working as is.  Your project directory is /home/myuser/apps/battleshiprl. You need to make sure that all files you create and work you do is done in that directory.


        Your task is to implement the full code based on the description and skeleton. Make sure you provide your response in the requested programming language. Your response should include the code for the entire file including proper import lines, but do not actually include the code from other files.

        You should print verbose output to the console to show the progress of the code execution. This will help the user understand what is happening at each step of the process.
                    
        If you are requested to make changes to an existing file, please provide the full file with the correction made as your response. Please keep your changes limited to what was requested and keep the rest of the code unchanged.
                    
        If you see possible errors that are outside of the scope of the request, please make a note of them using inline comments in the code. Provide additional explanation as needed in comments.
                    
        If you see additional improvements that could be made to the code, please make a note of them at the bottom of the code in the comments, but do not make the changes.
                    
        You may be occasionally asked to provide non-typical file responses, such as Markdown, Readme, ipynb, txt, etc. Please provide the file in the requested format instead of providing code that would generate the file.

        All of the code that you provide needs to be enclosed in a single markdown style code block with the language specified.
        Here is an example of what your response should look like:
        ```python
        # Your code goes here
        ```
        If it was javascript it would look like this:
        ```javascript
        // Your code goes here
        ```
                    
        Here is all of the code that has been created for the project so far:
        Code:
        # Code

## C:\Users\Machine81\Slazy\repo\battleshiprl\assets\sprites.py
```python
# /home/myuser/apps/battleshiprl/assets/sprites.py

"""
Sprite and asset manager for the Battleship game.

Implements:
    - SpriteManager class to load, resize, and manage game sprites
    - Methods for loading assets from disk if asset paths provided in constants
    - Default sprite surfaces (colored shapes) if images missing or not specified

Usage:
    manager = SpriteManager()
    sprite = manager.get_sprite("ship")   # or "hit", "miss"
"""

import os
import pygame
from game.constants import (
    CELL_SIZE, SPRITE_SHIP, SPRITE_HIT, SPRITE_MISS,
    COLOR_SHIP, COLOR_HIT, COLOR_MISS
)

ASSET_PATH = os.path.join(os.path.dirname(__file__), '')  # dir of this file

class SpriteManager:
    """
    Loads and manages sprite surfaces for the Battleship game.
    If any sprite asset is missing, falls back to generating a colored pygame Surface.
    """
    def __init__(self):
        self.sprites = {
            "ship": self._load_or_create_sprite("ship", SPRITE_SHIP, base_color=COLOR_SHIP),
            "hit": self._load_or_create_sprite("hit", SPRITE_HIT, base_color=COLOR_HIT),
            "miss": self._load_or_create_sprite("miss", SPRITE_MISS, base_color=COLOR_MISS),
        }

    def _load_or_create_sprite(self, key, path, base_color):
        """
        Tries to load an image asset from path; if not found, create colored Surface.
        Returns a pygame.Surface (CELL_SIZE x CELL_SIZE).
        """
        surface = None
        if path is not None:
            full_path = path
            if not os.path.isabs(path):
                full_path = os.path.join(ASSET_PATH, path)
            if os.path.exists(full_path):
                try:
                    img = pygame.image.load(full_path)
                    surface = pygame.transform.smoothscale(img, (CELL_SIZE, CELL_SIZE)).convert_alpha()
                    print(f"[SpriteManager] Loaded '{key}' sprite from '{full_path}'")
                except Exception as e:
                    print(f"[SpriteManager] Error loading image '{full_path}': {e}")
        if surface is None:
            # Generate default surface
            surface = pygame.Surface((CELL_SIZE, CELL_SIZE), pygame.SRCALPHA)
            if key == "ship":
                pygame.draw.rect(surface, base_color, surface.get_rect(), border_radius=8)
            elif key == "hit":
                surface.fill((0, 0, 0, 0))  # Transparent background
                pygame.draw.circle(surface, base_color, (CELL_SIZE // 2, CELL_SIZE // 2), CELL_SIZE // 3)
            elif key == "miss":
                surface.fill((0, 0, 0, 0))
                pygame.draw.circle(surface, base_color, (CELL_SIZE // 2, CELL_SIZE // 2), CELL_SIZE // 4)
            print(f"[SpriteManager] Created fallback sprite for '{key}' (color fill)")
        return surface

    def get_sprite(self, which):
        """
        Returns the pygame.Surface for 'ship', 'hit', or 'miss'
        """
        return self.sprites.get(which, None)

    def has_sprite(self, which):
        """
        Returns True if sprite asset (not fallback) exists for a given type.
        """
        s = self.sprites.get(which, None)
        # We don't track if loaded from disk vs generated by color here, but could add flag if needed.
        return s is not None

# ============== Test block ==================
if __name__ == "__main__":
    print("[sprites.py] Testing SpriteManager...")
    pygame.init()
    screen = pygame.display.set_mode((200, 80))
    mgr = SpriteManager()
    keys = ["ship", "hit", "miss"]
    positions = [(30, 30), (90, 30), (150, 30)]
    screen.fill((40, 50, 70))
    for k, pos in zip(keys, positions):
        surf = mgr.get_sprite(k)
        if surf:
            rect = surf.get_rect(center=pos)
            screen.blit(surf, rect)
    pygame.display.flip()
    print("[sprites.py] Displaying generated sprites (close to exit)...")
    waiting = True
    while waiting:
        for event in pygame.event.get():
            if event.type == pygame.QUIT:
                waiting = False
            elif event.type == pygame.KEYDOWN or event.type == pygame.MOUSEBUTTONDOWN:
                waiting = False
    pygame.quit()
    print("[sprites.py] SpriteManager test complete.")

# ===========================================
# Notes/Possible Improvements:
# - Could post-process loaded images (alpha conversion, colorization, etc.) for custom appearances.
# - Add support for per-ship-type images corresponding to each class of ship (by name).
# - Cache could be keyed by additional types for extensibility (e.g., sinking effect, water splash).
# - Folder structure for assets could be made more robust/configurable.
# - Default generated sprites can be improved aesthetically (gradients, stroke, etc.).
# ===========================================
```

## C:\Users\Machine81\Slazy\repo\battleshiprl\game\battleship.py
```python
# /home/myuser/apps/battleshiprl/game/battleship.py

import random
import numpy as np

from game.constants import (
    BOARD_SIZE, SHIPS, SHIP_NAMES, SHIP_SIZES, NUM_SHIPS,
    CELL_EMPTY, CELL_SHIP, CELL_HIT, CELL_MISS, CELL_SUNK,
    GAME_ONGOING, GAME_WIN, GAME_LOSE, GAME_DRAW,
    VERBOSE, RANDOM_SEED
)

if RANDOM_SEED is not None:
    random.seed(RANDOM_SEED)
    np.random.seed(RANDOM_SEED)

class Board:
    """
    Represents a single Battleship board.
    - self.state: 2D np.array (int), of size BOARD_SIZE x BOARD_SIZE
         0 = empty, 1 = ship, 2 = hit, 3 = miss, 4 = sunk
    - self.ships: list of dicts {'name', 'size', 'positions', 'hits'}
          'positions': list of (x, y) tuples covered by ship
          'hits': set of coordinates hit
    - self.ships_grid: 2D np.array of ship idx, -1 = no ship
    """
    def __init__(self):
        self.state = np.full((BOARD_SIZE, BOARD_SIZE), CELL_EMPTY, dtype=np.int8)
        self.ships = []
        self.ships_grid = np.full((BOARD_SIZE, BOARD_SIZE), -1, dtype=np.int8)

    def reset(self):
        """Resets board and places ships randomly"""
        self.state[:, :] = CELL_EMPTY
        self.ships = []
        self.ships_grid[:, :] = -1
        self._place_all_ships_randomly()

    def _place_all_ships_randomly(self):
        """Randomly places SHIPS on the board - no overlaps, all in bounds."""
        for idx, (ship_name, ship_size) in enumerate(zip(SHIP_NAMES, SHIP_SIZES)):
            placed = False
            tries = 0
            while not placed:
                orientation = random.choice(['H', 'V'])
                if orientation == 'H':
                    x = random.randint(0, BOARD_SIZE - ship_size)
                    y = random.randint(0, BOARD_SIZE - 1)
                    coords = [(x+i, y) for i in range(ship_size)]
                else:
                    x = random.randint(0, BOARD_SIZE - 1)
                    y = random.randint(0, BOARD_SIZE - ship_size)
                    coords = [(x, y+i) for i in range(ship_size)]
                # Check overlap
                if all(self.ships_grid[cy][cx] == -1 for (cx, cy) in coords):
                    # Place ship
                    ship = {
                        'name': ship_name,
                        'size': ship_size,
                        'positions': coords,
                        'hits': set()
                    }
                    self.ships.append(ship)
                    for (cx, cy) in coords:
                        self.state[cy][cx] = CELL_SHIP
                        self.ships_grid[cy][cx] = idx
                    if VERBOSE:
                        print(f"[Board] Placed ship '{ship_name}' at {coords} ({orientation})")
                    placed = True
                tries += 1
                if tries > 1000:
                    raise RuntimeError(f"[Board] Failed to place ship {ship_name} after 1000 tries -- placement logic error.")

    def fire(self, x, y):
        """
        Fires a shot at cell (x,y).
        Returns: (result: str, sunk_ship_name or None)
            result: 'miss', 'hit', or 'sunk'
            sunk_ship_name: Name if sunk, else None
        """
        # Out-of-bounds check
        if not (0 <= x < BOARD_SIZE and 0 <= y < BOARD_SIZE):
            if VERBOSE:
                print(f"[Board] Shot out of bounds ({x},{y}) ignored.")
            return 'miss', None

        cell = self.state[y][x]
        if cell == CELL_EMPTY:
            self.state[y][x] = CELL_MISS
            if VERBOSE:
                print(f"[Board] Shot at ({x},{y}): miss")
            return 'miss', None
        elif cell in (CELL_SHIP,): # Only can hit a fresh ship cell
            ship_idx = self.ships_grid[y][x]
            self.state[y][x] = CELL_HIT
            ship = self.ships[ship_idx]
            ship['hits'].add((x, y))
            if VERBOSE:
                print(f"[Board] Shot at ({x},{y}): hit on {ship['name']}")
            if len(ship['hits']) == ship['size']:
                # Sunk
                for (sx, sy) in ship['positions']:
                    self.state[sy][sx] = CELL_SUNK
                if VERBOSE:
                    print(f"[Board] Sunk {ship['name']} at {ship['positions']}")
                return 'sunk', ship['name']
            else:
                return 'hit', None
        elif cell in (CELL_HIT, CELL_MISS, CELL_SUNK):
            # Already fired upon
            if VERBOSE:
                print(f"[Board] Shot at ({x},{y}): already targeted")
            return 'miss', None

    def all_ships_sunk(self):
        """True if all ships have all their positions hit"""
        return all(len(ship['hits']) == ship['size'] for ship in self.ships)

    def available_shots(self):
        """List of (x, y) pairs where shots may be fired (i.e., not hit or miss or sunk)"""
        avail = []
        for y in range(BOARD_SIZE):
            for x in range(BOARD_SIZE):
                if self.state[y, x] in (CELL_EMPTY, CELL_SHIP):
                    avail.append((x, y))
        return avail

    def observation(self, reveal_ships=False):
        """
        Returns a representation for the RL agent:
        - If reveal_ships=False: only shows hits/misses
        - If reveal_ships=True: full true board (for debugging or self-board)
        Returns: 2D numpy array [BOARD_SIZE x BOARD_SIZE]
        """
        if reveal_ships:
            return self.state.copy()
        else:
            obs = self.state.copy()
            # Hide ships
            obs[obs == CELL_SHIP] = CELL_EMPTY
            return obs

    def clone(self):
        """Returns a deep copy of board (for DQN training transitions, if needed)"""
        import copy
        c = Board()
        c.state = np.copy(self.state)
        c.ships = copy.deepcopy(self.ships)
        c.ships_grid = np.copy(self.ships_grid)
        return c


class Battleship:
    """
    Controls a full two-player game of Battleship with Board boards.
    Each player has:
      - own_board (their ships)
      - target_board (their shots on opponent's ships)
    
    Usage: 
      bs = Battleship()
      obs0, obs1 = bs.reset()
      ... Then take turns with bs.step(player_idx, action) ...
    """
    def __init__(self):
        self.boards = [Board(), Board()]  # index 0/1 = player 0/1's own ships
        self.shots = [np.full((BOARD_SIZE, BOARD_SIZE), CELL_EMPTY, dtype=np.int8) for _ in range(2)]
        self.turn = 0  # Player 0 starts
        self.done = False
        self.winner = None

    def reset(self):
        """Resets both player boards and game state, does new placement"""
        if VERBOSE: print("[Game] Resetting game and placing ships.")
        for i in (0, 1):
            self.boards[i].reset()
            self.shots[i][:] = CELL_EMPTY
        self.turn = 0
        self.done = False
        self.winner = None
        return self.get_observation(0), self.get_observation(1)

    def get_observation(self, player_idx):
        """
        Returns full obs tuple for given agent:
          (own_board, target_board)
        Both are 2D np arrays, own_board fully revealed, target_board only what the agent knows.
        """
        own_board_obs = self.boards[player_idx].observation(reveal_ships=True)
        # Target board: only hit/miss/sunk where the player fired
        target_obs = self.shots[player_idx].copy()
        return own_board_obs, target_obs

    def get_legal_actions(self, player_idx):
        """Returns available (x, y) to fire at (on opponent's board)"""
        avail = []
        target = self.shots[player_idx]
        for y in range(BOARD_SIZE):
            for x in range(BOARD_SIZE):
                if target[y, x] == CELL_EMPTY:
                    avail.append((x, y))
        return avail

    def step(self, player_idx, action):
        """
        Agent (player_idx) takes action (x, y) at opponent.
        Returns:
            next_obs (own, target board),
            reward,
            done,
            info (dict: hit/miss/sunk, ship name, who won, ...)
        """
        assert not self.done, "Game is over"
        assert player_idx == self.turn, "It's not this player's turn!"
        opponent_idx = 1 - player_idx
        x, y = action

        # Fire shot at opponent's board
        result, sunk_ship = self.boards[opponent_idx].fire(x, y)
        # Mark in this player's shot tracking board
        if result == 'hit':
            self.shots[player_idx][y, x] = CELL_HIT
            reward = 1.0
        elif result == 'miss':
            self.shots[player_idx][y, x] = CELL_MISS
            reward = -0.25
        elif result == 'sunk':
            self.shots[player_idx][y, x] = CELL_HIT  # could keep as HIT/SUNK...
            reward = 2.0
        else:  # Shouldn't happen
            self.shots[player_idx][y, x] = CELL_MISS
            reward = 0.0

        info = {
            "result": result,
            "sunk_ship": sunk_ship,
            "player": player_idx,
            "opponent": opponent_idx
        }

        # Win/loss check
        if self.boards[opponent_idx].all_ships_sunk():
            self.done = True
            self.winner = player_idx
            if VERBOSE:
                print(f"[Game] Player {player_idx} wins! All opponent ships sunk.")
            reward += 10.0  # Major reward for victory
            info['done'] = True
            info['win'] = True
            info['winner'] = player_idx
        else:
            info['done'] = False
            info['win'] = False

        # Turn switch
        if not self.done:
            self.turn = opponent_idx

        # For training: negative reward if shooting at an already fired cell
        if result == 'miss' and (self.shots[player_idx][y, x] != CELL_EMPTY):
            reward = -1.0

        next_obs = self.get_observation(player_idx)
        return next_obs, reward, self.done, info

    def legal_action_mask(self, player_idx):
        """
        Returns flatten mask for each cell: 1 if available for shooting, else 0
        """
        mask = (self.shots[player_idx].flatten() == CELL_EMPTY).astype(np.float32)
        return mask

    def is_over(self):
        """True if game is over"""
        return self.done

    def get_winner(self):
        """Returns 0 or 1 if a winner; None if still ongoing"""
        return self.winner

    def render_debug(self):
        """Prints both boards to the console for debugging (text version)."""
        print("==== Player 0 Board ====")
        print(self._board_str(0))
        print("==== Player 1 Board ====")
        print(self._board_str(1))
        print("")

    def _board_str(self, idx):
        mapping = {
            CELL_EMPTY: ".",
            CELL_SHIP: "O",
            CELL_MISS: "*",
            CELL_HIT: "X",
            CELL_SUNK: "#"
        }
        arr = self.boards[idx].state
        rows = []
        for y in range(BOARD_SIZE):
            row = ""
            for x in range(BOARD_SIZE):
                row += mapping.get(arr[y, x], "?")
            rows.append(row)
        return "\n".join(rows)

# ================================================
# Notes/Possible Improvements for Future:
# - We could make Board keep track of "untouched" ship locations for partial reward shaping.
# - Policy could be given action mask for invalid moves for easier learning.
# - Could support different ship layouts or sizes.
# - The reward scheme might be adjusted in future for better RL stability.
# - Fast batch vectorization is possible for multi-game play, but not here.
# ================================================
```

## C:\Users\Machine81\Slazy\repo\battleshiprl\game\constants.py
```python
"""
Constants for the Battleship game and RL training
"""

# ===== BOARD SETTINGS =====
BOARD_SIZE = 10  # 10x10 board

# Dictionary of ship names and sizes, standard Battleship set
SHIPS = {
    'Carrier': 5,
    'Battleship': 4,
    'Cruiser': 3,
    'Submarine': 3,
    'Destroyer': 2,
}
SHIP_NAMES = list(SHIPS.keys())
SHIP_SIZES = list(SHIPS.values())
NUM_SHIPS = len(SHIPS)

# Cell states for the game board
CELL_EMPTY = 0
CELL_SHIP = 1
CELL_HIT = 2
CELL_MISS = 3
CELL_SUNK = 4  # for sunk ship segments

# ===== GAME STATES =====
GAME_ONGOING = 0
GAME_WIN = 1
GAME_LOSE = 2
GAME_DRAW = 3

# ===== RL AGENT PARAMETERS =====

# DQN Parameters
DQN_LEARNING_RATE = 1e-4
DQN_GAMMA = 0.99
DQN_EPSILON_START = 1.0
DQN_EPSILON_END = 0.1
DQN_EPSILON_DECAY = 0.995
DQN_BATCH_SIZE = 64
DQN_REPLAY_SIZE = 10000
DQN_UPDATE_FREQ = 1
DQN_TARGET_UPDATE_FREQ = 500

# Policy Gradient Parameters
PG_LEARNING_RATE = 1e-4
PG_GAMMA = 0.99
PG_BATCH_SIZE = 32

# General Training Params
NUM_EPISODES = 10000
VISUALIZE_EVERY = 100

# ===== COLORS =====

# RGB color tuples
COLOR_BG = (24, 29, 37)  # Very dark blue
COLOR_GRID = (50, 50, 60)
COLOR_SHIP = (100, 160, 210)
COLOR_HIT = (218, 41, 28)      # Red for hit
COLOR_MISS = (210, 210, 210)   # Light gray for miss
COLOR_SUNK = (40, 40, 40)      # Dark for sunk ship segment
COLOR_TEXT = (255, 255, 255)
COLOR_METRICS_BG = (35, 36, 50)
COLOR_METRICS_ACCENT = (60, 190, 105)

# For per-ship alternate colors
SHIP_COLORS = [
    (100, 160, 210),  # Carrier
    (160, 120, 200),  # Battleship
    (140, 180, 120),  # Cruiser
    (207, 153, 86),   # Submarine
    (156, 159, 190),  # Destroyer
]

# ===== DISPLAY SETTINGS =====
MARGIN = 5  # Gap between board sections
CELL_SIZE = 32  # Size of one cell in pixels

BOARD_PIXEL_SIZE = BOARD_SIZE * CELL_SIZE  # 320
BOARD_PADDING = 10  # px around board

# The screen displays two boards (player & opponent) side by side
SCREEN_WIDTH = 2 * (BOARD_PIXEL_SIZE + 2 * BOARD_PADDING) + MARGIN
SCREEN_HEIGHT = BOARD_PIXEL_SIZE + 2 * BOARD_PADDING + 220  # Extra vertical for metrics

# Bottom section for metrics visualization
METRICS_PANEL_HEIGHT = 200

# ===== FONTS =====
FONT_NAME = 'freesansbold.ttf'
FONT_SIZE_BOARD = 20
FONT_SIZE_METRICS = 18

# ===== METRICS SETTINGS =====
REWARD_AVG_WINDOW = 100
WINRATE_AVG_WINDOW = 100

# Visualization curve colors
CURVE_DQN = (60, 165, 220)     # Blue
CURVE_PG = (232, 125, 66)      # Orange
CURVE_WINRATE = (78, 196, 113) # Green

# ===== SPRITE PATHS (if any) =====
SPRITE_SHIP = None   # Set to asset file if using images instead of solid colors
SPRITE_HIT = None
SPRITE_MISS = None

# ===== RANDOM SEED (optional for reproducibility) =====
RANDOM_SEED = 42

# ===== DEBUGGING / VERBOSE =====
VERBOSE = True

# === END OF CONSTANTS ===

# Notes:
# - The above ship colors are mapped in order to the SHIP_NAMES list.
# - If you want to use sprite assets, update the SPRITE_* variables and ensure assets are found in 'assets/'.
# - VERBOSE can be toggled to show more/less information in console or logs.
# - Standard Battleship ship list is used (5 ships).
```

## C:\Users\Machine81\Slazy\repo\battleshiprl\main.py
```python
# /home/myuser/apps/battleshiprl/main.py

"""
Main entry point for Battleship RL
- Initializes Pygame and all subsystems
- Creates the two RL agents (DQN & Policy Gradient)
- Sets up Trainer and kicks off training
- Provides CLI options for demo, training, or evaluation runs
- Handles startup, exit, and intensive error reporting

MODIFIED:
- Runs in headless mode (no visualization) by default, for improved training stability and error avoidance.
- Visualization/trainer/metrics errors are caught and printed.
- Reduces default number of episodes for easier testing.
- Training progress is verbose in the console.
"""

import sys
import argparse
import os
import traceback

# ==== Project Imports ====
from game.constants import (
    SCREEN_WIDTH,
    SCREEN_HEIGHT,
    VERBOSE,
    NUM_EPISODES,
    VISUALIZE_EVERY
)
from models.dqn_agent import DQNAgent
from models.policy_gradient_agent import PolicyGradientAgent
from training.trainer import BattleshipTrainer

try:
    import pygame
except ImportError:
    print("[main.py] ERROR: Pygame must be installed (try: pip install pygame)")
    sys.exit(1)

DEFAULT_EPISODES = 30  # For test/dev. Increase as needed.
DEFAULT_HEADLESS = True

def parse_args():
    parser = argparse.ArgumentParser(
        description="Battleship RL - DQN vs Policy Gradient Self-Play Trainer",
        epilog="Example: python main.py --episodes 20000"
    )
    parser.add_argument("--episodes", type=int, default=DEFAULT_EPISODES, help="Number of training episodes")
    parser.add_argument("--visualize-every", type=int, default=VISUALIZE_EVERY, help="Visualize every N episodes")
    parser.add_argument("--headless", action='store_true', default=DEFAULT_HEADLESS, help="Disable Pygame visualization (run headless). DEFAULT: True")
    parser.add_argument("--demo", type=int, default=0, help="Run demo games (visualize trained agents for N games, no training)")
    parser.add_argument("--checkpoint-dir", default=None, help="Directory to load/save agent checkpoints")
    parser.add_argument("--quiet", action='store_true', help="Set VERBOSE=False (minimal console output)")
    parser.add_argument("--version", action='version', version="BattleshipRL 1.0")
    return parser.parse_args()

def setup_agents(checkpoint_dir=None):
    """Create and (optionally) load checkpointed agents."""
    dqn_agent = DQNAgent(name="DQN-Agent")
    pg_agent = PolicyGradientAgent(name="PolicyGrad-Agent")
    # Load weights if specified and found
    if checkpoint_dir and os.path.isdir(checkpoint_dir):
        dqn_ckpt = os.path.join(checkpoint_dir, "dqn_weights.pt")
        pg_ckpt = os.path.join(checkpoint_dir, "pg_weights.pt")
        if os.path.exists(dqn_ckpt):
            print("[main.py] Loading DQN weights from", dqn_ckpt)
            dqn_agent.load(dqn_ckpt)
        if os.path.exists(pg_ckpt):
            print("[main.py] Loading PG weights from", pg_ckpt)
            pg_agent.load(pg_ckpt)
    return dqn_agent, pg_agent

def save_agents(dqn, pg, checkpoint_dir):
    """Save current agent weights."""
    os.makedirs(checkpoint_dir, exist_ok=True)
    dqn_path = os.path.join(checkpoint_dir, "dqn_weights.pt")
    pg_path = os.path.join(checkpoint_dir, "pg_weights.pt")
    print(f"[main.py] Saving DQN agent to {dqn_path}")
    dqn.save(dqn_path)
    print(f"[main.py] Saving Policy Gradient agent to {pg_path}")
    pg.save(pg_path)

def main():
    args = parse_args()
    # Set verbosity
    global VERBOSE
    if args.quiet:
        VERBOSE = False
    print("========== Battleship RL (HEADLESS) ==========")
    print(f"[main.py] Episodes: {args.episodes}   |  Visualize every {args.visualize_every} | Headless: {args.headless}")
    if args.demo:
        print(f"[main.py] *** Demo mode: running {args.demo} visualized demo game(s). ***")

    # Enforce headless mode unless demo explicitly set
    # (args.demo implies visualization, so if demo==0, force headless)
    if not args.demo:
        args.headless = True

    screen = None

    if not args.headless:
        print("[main.py] Attempting to initialize Pygame display for visualization ...")
        try:
            pygame.init()
            screen = pygame.display.set_mode((SCREEN_WIDTH, SCREEN_HEIGHT))
            pygame.display.set_caption("BattleshipRL - RL Self-Play Training")
        except Exception as e:
            print("[main.py] [ERROR] Failed to initialize Pygame. Disabling visualization. Exception:")
            traceback.print_exc()
            print("[main.py] Continuing training with headless mode.")
            screen = None
            args.headless = True
    else:
        print("[main.py] Running in headless mode (no Pygame display)")

    # Construct agents
    dqn_agent, pg_agent = setup_agents(checkpoint_dir=args.checkpoint_dir)

    # Set up the trainer
    try:
        trainer = BattleshipTrainer(
            dqn_agent,
            pg_agent,
            screen=screen,
            visualize_every=args.visualize_every,
            num_episodes=args.episodes
        )
    except Exception as tr_ex:
        print("[main.py] [ERROR] Exception during Trainer construction -- disabling visualization and retrying...")
        traceback.print_exc()
        trainer = BattleshipTrainer(
            dqn_agent,
            pg_agent,
            screen=None,
            visualize_every=args.visualize_every,
            num_episodes=args.episodes
        )

    try:
        if args.demo:
            print("[main.py] Running demo/visualization only (no training)...")
            try:
                trainer.play_demo(num_games=args.demo)
                print("[main.py] Demo complete.")
            except Exception as viz_ex:
                print("[main.py] [ERROR] Visualization failed in demo mode. Exception:")
                traceback.print_exc()
                print("[main.py] Exiting demo.")
        else:
            print("[main.py] Starting headless training loop (no display)...")
            # Instead of visualizing, we print stats every N (e.g., 5) episodes for debugging
            stats_every = max(1, args.episodes // 10)
            for ep in range(1, args.episodes + 1):
                trainer.episode = ep
                trainer.agent0_is_dqn = (ep % 2 == 1)
                reward_dqn, reward_pg, winner_idx, ep_steps, loss_dqn, loss_pg = trainer.run_episode()
                # Winrate trace (approx rolling rate)
                if ep % stats_every == 0 or ep == 1 or ep == args.episodes:
                    print(f"[main.py] EP {ep:4d} | "
                          f"DQN Reward: {reward_dqn: .2f}, PG Reward: {reward_pg: .2f} | "
                          f"DQN Loss: {loss_dqn if loss_dqn else 0:.4f}, PG Loss: {loss_pg if loss_pg else 0:.4f} | "
                          f"DQN wrate: {dqn_agent.get_winrate():.2%} | PG wrate: {pg_agent.get_winrate():.2%} | "
                          f"Steps: {ep_steps}, Winner: {'DQN' if winner_idx==0 else 'PG'}")
                # Decay epsilon/exploration
                dqn_agent.decay_epsilon(game_end=True)
                # Update stats for rolling metrics
                dqn_is_winner = winner_idx == 0 if trainer.agent0_is_dqn else winner_idx == 1
                dqn_agent.stats_update(reward_dqn, win=dqn_is_winner)
                pg_agent.stats_update(reward_pg, win=not dqn_is_winner)
            print("[main.py] Headless training complete.")
            if args.checkpoint_dir:
                save_agents(dqn_agent, pg_agent, args.checkpoint_dir)
    except KeyboardInterrupt:
        print("\n[main.py] Caught KeyboardInterrupt: shutting down cleanly.")
        if args.checkpoint_dir:
            save_agents(dqn_agent, pg_agent, args.checkpoint_dir)
    except Exception as ex:
        print("\n[main.py] ERROR caught in main training loop.")
        traceback.print_exc()
        if args.checkpoint_dir:
            print("[main.py] Last attempt to save agent weights...")
            save_agents(dqn_agent, pg_agent, args.checkpoint_dir)
        print("[main.py] Exiting due to error.")
        sys.exit(2)
    finally:
        # Clean shutdown for Pygame
        if not args.headless and screen is not None:
            print("[main.py] Shutting down Pygame...")
            pygame.quit()
        print("[main.py] Done.")

if __name__ == "__main__":
    main()

# ===============================
# Notes/Possible Improvements:
# - Optionally provide a --visualize argument to override default headless mode.
# - Visualization, if needed, could be run from a separate script/process (e.g., for TensorBoard or a web dashboard).
# - Main training loop can be abstracted out to avoid code duplication.
# - For large episode runs, support log-to-file for metrics.
# - Print more advanced stats (average lengths, Q-values, policy entropy, etc).
# ===============================
```

## C:\Users\Machine81\Slazy\repo\battleshiprl\models\dqn_agent.py
```python
# /home/myuser/apps/battleshiprl/models/dqn_agent.py

"""
Deep Q-Network agent for Battleship.

Implements a DQN agent:
- Neural networks (policy & target)
- Epsilon-greedy action selection
- Experience replay
- Q-learning updates
- Target net sync
- Exposes methods:
    * select_action()
    * observe()
    * train_step()
    * update_target()
    * stats for visualization
"""

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import random

from game.constants import (
    BOARD_SIZE, DQN_LEARNING_RATE, DQN_GAMMA,
    DQN_EPSILON_START, DQN_EPSILON_END, DQN_EPSILON_DECAY,
    DQN_BATCH_SIZE, DQN_REPLAY_SIZE, DQN_TARGET_UPDATE_FREQ, VERBOSE
)
from models.neural_nets import DQNNetwork, make_input_tensor
from models.replay_buffer import ReplayBuffer

class DQNAgent:
    def __init__(self, name="DQN", device=None):
        self.name = name
        self.board_size = BOARD_SIZE
        self.lr = DQN_LEARNING_RATE
        self.gamma = DQN_GAMMA
        self.epsilon = DQN_EPSILON_START
        self.epsilon_min = DQN_EPSILON_END
        self.epsilon_decay = DQN_EPSILON_DECAY
        self.batch_size = DQN_BATCH_SIZE
        self.target_update_freq = DQN_TARGET_UPDATE_FREQ

        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")
        if VERBOSE: print(f"[{self.name}] Using device: {self.device}")

        self.policy_net = DQNNetwork(board_size=self.board_size).to(self.device)
        self.target_net = DQNNetwork(board_size=self.board_size).to(self.device)
        self.target_net.load_state_dict(self.policy_net.state_dict())
        self.target_net.eval()

        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=self.lr)
        self.replay = ReplayBuffer(DQN_REPLAY_SIZE)

        self.step_count = 0
        self.stats = {
            "loss": [],
            "epsilon": [],
            "reward": [],
            "wins": 0,
            "games": 0
        }

    def select_action(self, own_board, target_board, legal_mask, eval_mode=False):
        """Chooses action using epsilon-greedy strategy.
        own_board, target_board: np.arrays
        legal_mask: np.array of shape [BOARD_SIZE*BOARD_SIZE], 1=legal, 0=not
        eval_mode: no random (for evaluation/visualization)
        Returns: (x, y) tuple of cell to fire
        """
        flat_size = self.board_size * self.board_size
        mask = torch.tensor(legal_mask, dtype=torch.bool, device=self.device)
        use_random = (not eval_mode) and (random.random() < self.epsilon)
        with torch.no_grad():
            inp = make_input_tensor(own_board, target_board).unsqueeze(0).to(self.device)
            qvalues = self.policy_net(inp).squeeze(0)  # shape: [100]
            # Mask invalid actions to -inf so they never get picked by argmax
            qvalues_masked = qvalues.clone()
            qvalues_masked[~mask] = -1e9
        if use_random:
            legal_idxs = mask.nonzero(as_tuple=False).view(-1)
            action_idx = random.choice(legal_idxs.tolist())
            if VERBOSE:
                print(f"[{self.name}] Random action: idx={action_idx}")
        else:
            # Exploit: pick best Q among available
            action_idx = torch.argmax(qvalues_masked).item()
            if VERBOSE:
                print(f"[{self.name}] DQN action select: greedy idx={action_idx} Q={qvalues[action_idx]:.2f}")
        x = action_idx % self.board_size
        y = action_idx // self.board_size
        return (x, y)

    def observe(self, state, action, reward, next_state, done):
        """Stores experience tuple in replay buffer."""
        # action can be (x, y), store as flat index
        x, y = action
        action_idx = x + y * self.board_size
        # Each obs is (own_board, target_board)
        self.replay.add(state, action_idx, reward, next_state, done)

    def train_step(self):
        """Performs a single batched DQN update if enough samples exist."""
        # Only train if we have enough minibatch
        if not self.replay.ready(self.batch_size):
            return None

        self.policy_net.train()
        states, actions, rewards, next_states, dones = self.replay.sample(self.batch_size)
        # All: (batch, ...)
        # Prepare tensors
        # Convert obs to [batch, 10, H, W]
        inps = []
        next_inps = []
        for i in range(self.batch_size):
            s_own, s_tgt = states[i]
            n_own, n_tgt = next_states[i]
            inps.append(make_input_tensor(s_own, s_tgt))
            next_inps.append(make_input_tensor(n_own, n_tgt))
        inp_tensor = torch.stack(inps).to(self.device)
        next_inp_tensor = torch.stack(next_inps).to(self.device)
        action_tensor = torch.tensor(actions, dtype=torch.long, device=self.device)
        reward_tensor = torch.tensor(rewards, dtype=torch.float32, device=self.device)
        done_tensor = torch.tensor(dones, dtype=torch.float32, device=self.device)

        # Q(s,a)
        qvals = self.policy_net(inp_tensor)
        qvals = qvals.gather(1, action_tensor.unsqueeze(1)).squeeze(1)
        # Q_target(s', a')
        with torch.no_grad():
            # Max Q of next state (across all actions)
            q_next = self.target_net(next_inp_tensor).max(1)[0]
            q_target = reward_tensor + self.gamma * q_next * (1.0 - done_tensor)
        # Loss: MSE
        loss = nn.functional.mse_loss(qvals, q_target)
        # Optimize
        self.optimizer.zero_grad()
        loss.backward()
        nn.utils.clip_grad_norm_(self.policy_net.parameters(), 1.0)  # gradient clip
        self.optimizer.step()
        # Logging
        self.stats["loss"].append(loss.item())
        # Target net update
        if self.step_count % self.target_update_freq == 0:
            self.update_target()
        self.step_count += 1
        if VERBOSE:
            print(f"[{self.name}] DQN train step: Loss={loss.item():.4f} | epsilon={self.epsilon:.3f}")
        return loss.item()

    def update_target(self):
        """Synchronizes target network from policy network."""
        self.target_net.load_state_dict(self.policy_net.state_dict())
        if VERBOSE:
            print(f"[{self.name}] [DQN] Target network updated.")

    def decay_epsilon(self, game_end=False):
        """Decay epsilon after episode ends, not after every step."""
        if game_end:
            old_eps = self.epsilon
            self.epsilon = max(self.epsilon * self.epsilon_decay, self.epsilon_min)
            if VERBOSE and old_eps != self.epsilon:
                print(f"[{self.name}] Epsilon decayed: {old_eps:.4f} -> {self.epsilon:.4f}")
        self.stats["epsilon"].append(self.epsilon)

    def stats_update(self, reward, win=None):
        """Tracks running reward and win record (used by trainer)."""
        self.stats["reward"].append(reward)
        if win is not None:
            self.stats["games"] += 1
            if win:
                self.stats["wins"] += 1

    def get_winrate(self):
        """Returns rolling winrate (0..1) over played games."""
        games = self.stats["games"]
        wins = self.stats["wins"]
        if games == 0:
            return 0.0
        return wins / games

    def get_stats(self):
        """Returns a dict for plotting/metrics on user interface."""
        return {
            "loss": self.stats["loss"][-500:],  # last N
            "epsilon": self.stats["epsilon"][-500:],
            "reward": self.stats["reward"][-500:],
            "winrate": self.get_winrate(),
        }

    def save(self, path):
        """Saves net parameters to disk."""
        torch.save(self.policy_net.state_dict(), path)

    def load(self, path):
        """Loads net parameters from disk."""
        self.policy_net.load_state_dict(torch.load(path, map_location=self.device))
        self.target_net.load_state_dict(self.policy_net.state_dict())

# ================================
# Notes/Possible Improvements:
# - Could use Double DQN (predict next_action with policy net, evaluate w/ target net).
# - Could add prioritized replay sampling.
# - Optionally batch epsilon decay so it anneals only after new episode/game (currently supports).
# - For variable board sizes, parametrize sizes everywhere.
# - Clip rewards (optional).
# ================================

# ========== Test block ==========
if __name__ == "__main__":
    print("[dqn_agent.py] Running self-test...")
    # Fake board
    own = np.zeros((BOARD_SIZE, BOARD_SIZE), dtype=np.int8)
    tgt = np.zeros((BOARD_SIZE, BOARD_SIZE), dtype=np.int8)
    mask = np.ones(BOARD_SIZE * BOARD_SIZE, dtype=np.float32)
    agent = DQNAgent()
    # Add fake experiences
    for i in range(20):
        next_own = own.copy()
        next_tgt = tgt.copy()
        state = (own, tgt)
        next_state = (next_own, next_tgt)
        reward = float(np.random.randn())
        action = (i % BOARD_SIZE, (i // BOARD_SIZE) % BOARD_SIZE)
        done = i % 5 == 0
        agent.observe(state, action, reward, next_state, done)
        if i % 3 == 0:
            agent.train_step()
    print("[dqn_agent.py] Test complete.")
```

## C:\Users\Machine81\Slazy\repo\battleshiprl\models\neural_nets.py
```python
# /home/myuser/apps/battleshiprl/models/neural_nets.py

"""
Neural network architectures for RL agents
"""

import torch
import torch.nn as nn
import torch.nn.functional as F

from game.constants import BOARD_SIZE

# ================================
# Notes on Input Representation:
# - Each agent receives two 10x10 boards:
#     * own_board (full information / all ship locations) [for richer input, else ignore for pure "target-only" observation]
#     * target_board (what they know about the enemy - shows their own hits/misses/unknown)
# - Both are int-valued grid arrays; we'll use channel encoding:
#     - One-hot encoding: C channels, with one channel per cell type (empty/ship/miss/hit/sunk), OR
#     - Scalar (int) per cell (less ideal for conv)
# - We'll stack as input (batch, channels, H, W)
#
# - DQNNetwork outputs: Q-value for all possible actions (firing at each cell): 10x10 = 100 outputs
# - PolicyNetwork outputs: probability distribution over all 100 locations (using softmax with mask)
# ================================

def make_input_tensor(own_board, target_board):
    """
    Prepares the input tensor for the neural nets:
    - own_board and target_board are 2D np.arrays (BOARD_SIZE x BOARD_SIZE)
    Returns: torch.FloatTensor of shape [channels=10, BOARD_SIZE, BOARD_SIZE]
    Channels:
      - own_board: [empty, ship, hit, miss, sunk] (5 channels)
      - target_board: [empty, hit, miss, sunk, ignore] (5 channels)
    """
    # One-hot for both boards
    NUM_TYPES = 5   # empty, ship, hit, miss, sunk
    batch = []
    for arr in (own_board, target_board):
        arrch = []
        for i in range(NUM_TYPES):
            arrch.append((arr == i).astype('float32'))
        batch.extend(arrch)
    arr_stacked = torch.tensor(batch, dtype=torch.float32)
    return arr_stacked  # shape (10, 10, 10)

class DQNNetwork(nn.Module):
    """
    Neural network for DQN agent
    Input: tensor of shape (batch, channels, BOARD_SIZE, BOARD_SIZE)
    Output: Q-values for each possible action (batch, BOARD_SIZE * BOARD_SIZE)
        (100 outputs, one per valid firing cell)
    """
    def __init__(self, board_size=BOARD_SIZE, input_channels=10):
        super(DQNNetwork, self).__init__()
        self.board_size = board_size

        # Conv2D layers
        self.conv1 = nn.Conv2d(input_channels, 32, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, padding=1)

        self.flat_size = 64 * board_size * board_size
        self.fc1 = nn.Linear(self.flat_size, 512)
        self.fc2 = nn.Linear(512, board_size * board_size)

    def forward(self, x):
        # x shape: (batch, channels, board_size, board_size)
        x = F.relu(self.conv1(x))
        x = F.relu(self.conv2(x))
        x = F.relu(self.conv3(x))
        x = x.view(x.size(0), -1)
        x = F.relu(self.fc1(x))
        q = self.fc2(x)
        return q  # shape: (batch, board_size*board_size)

class PolicyNetwork(nn.Module):
    """
    Neural network for Policy Gradient agent
    Input: tensor of shape (batch, channels, BOARD_SIZE, BOARD_SIZE)
    Output: log-probabilities over actions (batch, BOARD_SIZE * BOARD_SIZE)
    (Softmax at output for valid actions; masking is handled outside this net)
    """
    def __init__(self, board_size=BOARD_SIZE, input_channels=10):
        super(PolicyNetwork, self).__init__()
        self.board_size = board_size

        self.conv1 = nn.Conv2d(input_channels, 32, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, padding=1)

        self.flat_size = 64 * board_size * board_size
        self.fc1 = nn.Linear(self.flat_size, 512)
        self.fc2 = nn.Linear(512, board_size * board_size)

    def forward(self, x):
        # x shape: (batch, channels, board_size, board_size)
        x = F.relu(self.conv1(x))
        x = F.relu(self.conv2(x))
        x = F.relu(self.conv3(x))
        x = x.view(x.size(0), -1)
        x = F.relu(self.fc1(x))
        logits = self.fc2(x)  # shape: (batch, board_size*board_size)
        return logits

# ================================
# Test block (run as __main__ to verify basic output shape)
# ================================
if __name__ == "__main__":
    import numpy as np
    print("[neural_nets.py] Running basic shape tests...")
    own = np.zeros((BOARD_SIZE, BOARD_SIZE), dtype=np.int8)
    own[0, 0] = 1  # Simulate a ship
    own[1, 3] = 2  # Simulate a hit on own
    tgt = np.zeros((BOARD_SIZE, BOARD_SIZE), dtype=np.int8)
    tgt[2, 4] = 2  # Simulate a hit on target
    tgt[5, 7] = 3  # Simulate a miss on target
    tinp = make_input_tensor(own, tgt).unsqueeze(0)  # Add batch dimension
    print(" Input tensor shape:", tinp.shape)

    net_q = DQNNetwork()
    qvals = net_q(tinp)
    print(" DQN output shape (should be [1, 100]):", qvals.shape)

    net_p = PolicyNetwork()
    logits = net_p(tinp)
    print(" PolicyNetwork output shape (should be [1, 100]):", logits.shape)
    print("[neural_nets.py] Test complete.")

# ================================
# Notes/Possible Improvements:
# - May experiment with more advanced input encoding (e.g., add last move, remaining ships, etc.)
# - May use Residual Convs for deeper models if underfitting.
# - Optionally allow agent settings to ignore own_board info for more challenging play.
# - Policy head could output log-softmax for direct use in loss.
# - Can add dropout or batch norm if overfitting encountered.
# ================================
```

## C:\Users\Machine81\Slazy\repo\battleshiprl\models\policy_gradient_agent.py
```python
# /home/myuser/apps/battleshiprl/models/policy_gradient_agent.py

"""
Policy Gradient agent for Battleship

Implements a Policy Gradient (PG) reinforcement learning agent:
- Network architecture to output action probabilities
- Experience collection and memory management
- Policy gradient loss computation with rewards-to-go
- Trajectory-based updates (updating after completed episodes)
- Handling legal/illegal moves with action masking

The class handles:
- Converting game states to neural network inputs
- Taking actions by sampling from the policy distribution
- Storing experience trajectories for episodic updates
- Computing policy gradient loss and updating weights
- Tracking performance metrics

PGAgent is meant for on-policy learning (REINFORCE style).
"""

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import random

from game.constants import (
    BOARD_SIZE, PG_LEARNING_RATE, PG_GAMMA, PG_BATCH_SIZE, VERBOSE
)
from models.neural_nets import PolicyNetwork, make_input_tensor

class PolicyGradientAgent:
    def __init__(self, name="PG", device=None):
        self.name = name
        self.board_size = BOARD_SIZE
        self.gamma = PG_GAMMA
        self.lr = PG_LEARNING_RATE
        self.batch_size = PG_BATCH_SIZE

        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")
        if VERBOSE: print(f"[{self.name}] Using device: {self.device}")

        self.policy_net = PolicyNetwork(board_size=self.board_size).to(self.device)
        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=self.lr)

        # Experience buffer for one episode ("trajectory")
        self.trajectory = []
        self.stats = {
            "loss": [],
            "reward": [],
            "wins": 0,
            "games": 0
        }

    def select_action(self, own_board, target_board, legal_mask, eval_mode=False):
        """
        Returns (x, y) action sampled from policy, given (own_board, target_board), and legal_mask.
        If eval_mode, always pick highest probability valid move (greedy).
        """
        inp = make_input_tensor(own_board, target_board).unsqueeze(0).to(self.device)  # (1, 10, 10, 10)
        with torch.no_grad():
            logits = self.policy_net(inp).squeeze(0)  # (100,)
        mask = torch.tensor(legal_mask, dtype=torch.bool, device=self.device)
        logits[~mask] = -1e9  # Mask illegal actions
        prob_dist = torch.softmax(logits, dim=-1)
        if eval_mode:
            action_idx = torch.argmax(prob_dist).item()
            if VERBOSE:
                print(f"[{self.name}] Greedy policy eval action idx={action_idx} with prob={prob_dist[action_idx]:.2f}")
        else:
            # Sample from the probabilistic policy among valid actions
            legal_probs = prob_dist[mask]
            legal_indices = mask.nonzero(as_tuple=False).view(-1)
            if legal_probs.sum().item() == 0:
                # No valid actions?? Choose random valid as backup (should not happen!)
                action_idx = random.choice(legal_indices.cpu().tolist())
                if VERBOSE: print(f"[{self.name}] (Warning) No valid actions found, picking random legal!")
            else:
                sampled_idx = torch.multinomial(legal_probs, 1).item()
                action_idx = legal_indices[sampled_idx].item()
                if VERBOSE:
                    print(f"[{self.name}] Stochastic policy action idx={action_idx} prob={prob_dist[action_idx]:.2f}")
        x = action_idx % self.board_size
        y = action_idx // self.board_size
        return (x, y)

    def observe(self, state, action, reward):
        """
        Stores trajectory step for policy gradient update.
        - state: tuple (own_board, target_board)
        - action: (x, y)
        - reward: float
        """
        x, y = action
        action_idx = x + y * self.board_size
        # Note: store obs as-is for later batching
        self.trajectory.append((state, action_idx, reward))

    def finish_episode_and_learn(self):
        """
        After an episode, process full trajectory and perform policy gradient update.
        Computes rewards-to-go and loss, then backpropagates.
        """
        if len(self.trajectory) == 0:
            return None  # Nothing to learn from yet

        # Prepare tensors for states and actions and rewards
        states, actions, rewards = zip(*self.trajectory)
        # Calculate rewards-to-go for every step (discounted sum from t to end)
        R = 0
        returns = []
        for r in reversed(rewards):
            R = r + self.gamma * R
            returns.insert(0, R)
        returns = torch.tensor(returns, dtype=torch.float32, device=self.device)
        # Normalize returns for variance reduction (optional, usually helps)
        returns = (returns - returns.mean()) / (returns.std() + 1e-8)

        # Build input tensors for all timesteps
        inp_tensors = []
        for state in states:
            own, tgt = state
            inp = make_input_tensor(own, tgt)
            inp_tensors.append(inp)
        inp_batch = torch.stack(inp_tensors).to(self.device)
        actions_tensor = torch.tensor(actions, dtype=torch.long, device=self.device)

        # Compute log-probs for all actions
        logits = self.policy_net(inp_batch)  # (N, 100)
        log_probs = torch.log_softmax(logits, dim=1)
        selected_logprobs = log_probs.gather(1, actions_tensor.unsqueeze(1)).squeeze(1)  # (N,)

        # Policy gradient loss (REINFORCE): negative for ascent
        loss = -(selected_logprobs * returns).mean()

        self.optimizer.zero_grad()
        loss.backward()
        nn.utils.clip_grad_norm_(self.policy_net.parameters(), 1.0)
        self.optimizer.step()

        self.stats["loss"].append(loss.item())
        self.trajectory = []  # Clear for next episode

        if VERBOSE:
            print(f"[{self.name}] Policy Gradient update: Loss={loss.item():.4f} [Episode steps={len(actions)}]")

        return loss.item()

    def stats_update(self, reward, win=None):
        """Tracks running reward and win record (used by trainer)"""
        self.stats["reward"].append(reward)
        if win is not None:
            self.stats["games"] += 1
            if win:
                self.stats["wins"] += 1

    def get_winrate(self):
        games = self.stats["games"]
        wins = self.stats["wins"]
        if games == 0:
            return 0.0
        return wins / games

    def get_stats(self):
        """Returns dict for visualization/metrics panel"""
        return {
            "loss": self.stats["loss"][-500:],  # last N
            "reward": self.stats["reward"][-500:],
            "winrate": self.get_winrate(),
        }

    def save(self, path):
        torch.save(self.policy_net.state_dict(), path)

    def load(self, path):
        self.policy_net.load_state_dict(torch.load(path, map_location=self.device))

# ================================
# Test block -- verify network forward and simple rollout
# ================================
if __name__ == "__main__":
    import numpy as np
    print("[policy_gradient_agent.py] Running standalone test...")
    own = np.zeros((BOARD_SIZE, BOARD_SIZE), dtype=np.int8)
    tgt = np.zeros((BOARD_SIZE, BOARD_SIZE), dtype=np.int8)
    mask = np.ones(BOARD_SIZE * BOARD_SIZE, dtype=np.float32)
    agent = PolicyGradientAgent()
    # Simulate a single episode
    for t in range(5):
        action = agent.select_action(own, tgt, mask, eval_mode=(t==0))
        reward = 1.0 if t == 4 else 0.0
        agent.observe((own.copy(), tgt.copy()), action, reward)
    agent.finish_episode_and_learn()
    print("[policy_gradient_agent.py] Test complete.")

# ================================
# Notes/Possible Improvements:
# - Add baseline network (e.g., value function) for variance reduction (A2C style).
# - Could support GAE (Generalized Advantage Estimation) for better bias-variance tradeoff.
# - Optionally, learn with actor-critic for improved learning efficiency.
# - Policy gradient can be improved by masking illegal moves directly in network outputs.
# - Could adapt learning rate annealing, entropy regularization, or other RL techniques.
# - Batch updates for multiple episodes at once if desired for improved stability.
# ================================
```

## C:\Users\Machine81\Slazy\repo\battleshiprl\models\replay_buffer.py
```python
# /home/myuser/apps/battleshiprl/models/replay_buffer.py

"""
Replay Buffer for storing experience tuples during reinforcement learning.
"""

import random
import numpy as np
from collections import deque

class ReplayBuffer:
    """
    Experience replay buffer for off-policy RL algorithms like DQN.

    Stores experiences of the form:
        (state, action, reward, next_state, done)
    where:
        - state: np.array (can be multidimensional)
        - action: int or tuple (can be index or coordinate)
        - reward: float
        - next_state: np.array
        - done: bool (terminal)
    
    Usage:
        buf = ReplayBuffer(capacity=10000)
        buf.add(state, action, reward, next_state, done)
        batch = buf.sample(batch_size=64)
    """

    def __init__(self, capacity):
        """Initialize a buffer with given capacity"""
        self.buffer = deque(maxlen=capacity)
        self.capacity = capacity

    def __len__(self):
        """Return the current size of internal buffer"""
        return len(self.buffer)
    
    def add(self, state, action, reward, next_state, done):
        """
        Add a single experience to the buffer.
        Copies of state/next_state are stored to prevent mutation bugs.
        """
        # Copy arrays to prevent outside mutation
        import copy
        state_c = copy.deepcopy(state)
        next_state_c = copy.deepcopy(next_state)
        self.buffer.append((state_c, action, reward, next_state_c, done))
    
    def sample(self, batch_size):
        """
        Sample a batch of experiences uniformly at random.

        Returns a tuple of arrays:
            (states, actions, rewards, next_states, dones)
        Where each is a list/numpy array of batch_size elements.
        """
        assert len(self.buffer) >= batch_size, "Not enough samples to sample batch_size"
        samples = random.sample(self.buffer, batch_size)
        states, actions, rewards, next_states, dones = zip(*samples)
        # Return as lists; conversion to np/tensor can be done by caller
        return list(states), list(actions), list(rewards), list(next_states), list(dones)
    
    def clear(self):
        """Empties the replay buffer."""
        self.buffer.clear()
    
    def ready(self, batch_size):
        """Returns True if the buffer contains at least batch_size elements, for sampling."""
        return len(self.buffer) >= batch_size

# ========== Testing section ==========
if __name__ == "__main__":
    print("[ReplayBuffer] Running test cases...")
    buf = ReplayBuffer(capacity=5)
    # Add 7 experiences (should overwrite oldest when full)
    for i in range(7):
        st = np.array([i])
        act = i
        rew = i * 1.1
        nst = np.array([i+1])
        done = (i == 6)
        buf.add(st, act, rew, nst, done)
        print(f"  Added experience {i}: (state={st}, action={act}, reward={rew}, next_state={nst}, done={done})")
        print(f"  Current buffer size: {len(buf)}")
    print("[ReplayBuffer] Buffer contents (oldest first):")
    for idx, item in enumerate(buf.buffer):
        print(f"  {idx}: {item}")
    if buf.ready(batch_size=3):
        print("[ReplayBuffer] Sampling batch of 3 experiences...")
        s, a, r, ns, d = buf.sample(batch_size=3)
        print("  Sampled states: ", s)
        print("  Sampled actions:", a)
        print("  Sampled rewards:", r)
        print("  Sampled next_states:", ns)
        print("  Sampled dones:  ", d)
    else:
        print("[ReplayBuffer] Not enough samples to sample batch.")

# ==============================
# Notes/Possible Improvements:
# - For large scale, can use python deque or numpy ring-buffer for efficiency.
# - Can add prioritization for prioritized replay (Schaul et al.) in future.
# - This implementation makes separate copies of arrays for safety, but for big obs, consider shallow copy.
# - If storing raw images, may need to optimize further for memory.
# ==============================
```

## C:\Users\Machine81\Slazy\repo\battleshiprl\training\trainer.py
```python
# /home/myuser/apps/battleshiprl/training/trainer.py

"""
Training system for Battleship RL agents

- Controls the training loop between two RL agents (DQN and Policy Gradient)
- Runs multiple episodes, switching first player to ensure fairness
- Tracks and logs episodic rewards, losses, win rates for each agent/model
- Handles periodic Pygame visualization: displays gameplay and training metrics
- Hooks up the renderer and metrics visualizer components
"""

import time
import numpy as np
import pygame

from game.constants import (
    NUM_EPISODES, VISUALIZE_EVERY, VERBOSE,
    SCREEN_WIDTH, SCREEN_HEIGHT, BOARD_SIZE, BOARD_PADDING, MARGIN,
    FONT_NAME, FONT_SIZE_METRICS
)
from game.battleship import Battleship
from models.dqn_agent import DQNAgent
from models.policy_gradient_agent import PolicyGradientAgent
from visualization.renderer import GameRenderer
from visualization.metrics import MetricsVisualizer
from assets.sprites import SpriteManager

class BattleshipTrainer:
    """
    Main controller for RL training loop, managing self-play, stats, and visualization.
    """
    def __init__(self,
                 dqn_agent,
                 pg_agent,
                 screen=None,
                 visualize_every=VISUALIZE_EVERY,
                 num_episodes=NUM_EPISODES):
        self.dqn = dqn_agent
        self.pg = pg_agent
        self.visualize_every = visualize_every
        self.num_episodes = num_episodes

        # Visualization
        self.screen = screen
        self.pygame_mode = screen is not None
        self.sprites = SpriteManager() if self.pygame_mode else None
        if self.pygame_mode:
            self.renderer = GameRenderer(self.screen, sprite_manager=self.sprites)
            self.metrics_viz = MetricsVisualizer(self.screen)
            pygame.display.set_caption("Battleship RL - Training & Learning Visualization")

        # For episode/metric tracking
        self.episode = 0
        self.episode_rewards = []
        self.episode_losses = []
        self.win_history = []  # Record tuple (0=win, 1=loss) for agent 0 (DQN) perspective

        # For agent alternation: who is player 0/1 in a given episode
        self.agent0_is_dqn = True

        # For timing
        self.last_vis_time = time.time()

    def run(self):
        """
        Main outer training loop.
        Alternates agent order for fairness.
        Trains DQN and PG simultaneously.
        Handles periodic visualization.
        """
        print("[Trainer] Starting training loop...")
        for ep in range(1, self.num_episodes + 1):
            self.episode = ep
            self.agent0_is_dqn = (ep % 2 == 1)  # Alternate who goes first: odd = DQN, even = PG
            reward_dqn, reward_pg, win_idx, ep_steps, loss_dqn, loss_pg = self.run_episode()
            # Track metrics
            if self.agent0_is_dqn:
                # Agent order: [0]=DQN, [1]=PG; winner index is in this order.
                dqn_is_winner = (win_idx == 0)
            else:
                # [0]=PG, [1]=DQN; invert
                dqn_is_winner = (win_idx == 1)
            self.dqn.stats_update(reward_dqn, win=dqn_is_winner)
            self.pg.stats_update(reward_pg, win=not dqn_is_winner)
            self.win_history.append(1 if dqn_is_winner else 0)
            # Decay epsilon only after (both agents') full episode/game
            self.dqn.decay_epsilon(game_end=True)

            if ep % self.visualize_every == 0 or ep == 1:
                self.visualize_game(last_episode_play=True, cur_ep=ep)
            else:
                if VERBOSE and ep % 10 == 0:
                    print(f"[Trainer] Episode {ep:4d} | DQN reward: {reward_dqn:6.2f} | PG reward: {reward_pg:6.2f} | "
                          f"DQN winrate: {self.dqn.get_winrate():.2%}")

        print("[Trainer] Training complete.")

    def run_episode(self):
        """
        Runs a full game: each agent controls a player, alternating shot turns.
        Returns: (dqn_total_reward, pg_total_reward, winner_idx, steps_taken, dqn_loss, pg_loss)
        """
        game = Battleship()
        obs0, obs1 = game.reset()
        # Assign agents in current order
        if self.agent0_is_dqn:
            agents = [self.dqn, self.pg]
            agent_names = ["DQN", "PG"]
        else:
            agents = [self.pg, self.dqn]
            agent_names = ["PG", "DQN"]

        states = [obs0, obs1]
        total_rewards = [0.0, 0.0]
        total_losses = [0.0, 0.0]
        steps = 0

        while not game.is_over():
            agent_idx = game.turn        # Player whose turn it is: 0 or 1
            o_board, t_board = game.get_observation(agent_idx)
            legal_mask = game.legal_action_mask(agent_idx)
            action = agents[agent_idx].select_action(o_board, t_board, legal_mask)
            # Step returns: (own_board, target_board), reward, done, info
            next_obs, reward, done, info = game.step(agent_idx, action)
            next_state = next_obs
            # For both agents, update stats as needed for RL training
            if isinstance(agents[agent_idx], DQNAgent):
                # DQN needs to observe (state, action, reward, next_state, done)
                agents[agent_idx].observe(states[agent_idx], action, reward, next_state, done)
                loss = agents[agent_idx].train_step()
                total_losses[agent_idx] += (loss if loss is not None else 0.0)
            elif isinstance(agents[agent_idx], PolicyGradientAgent):
                agents[agent_idx].observe(states[agent_idx], action, reward)
                # (Learn only at end of episode)
            states[agent_idx] = next_state
            total_rewards[agent_idx] += reward
            steps += 1

        # End of episode: PG agent(s) perform policy gradient update
        for idx, agent in enumerate(agents):
            if isinstance(agent, PolicyGradientAgent):
                loss_pg = agent.finish_episode_and_learn()
                total_losses[idx] += (loss_pg if loss_pg is not None else 0.0)

        winner = game.get_winner()  # 0 or 1, in (agents) order
        # Map cumulative stats to [DQN, PG]
        if self.agent0_is_dqn:
            dqn_total_reward = total_rewards[0]
            dqn_loss = total_losses[0]
            pg_total_reward = total_rewards[1]
            pg_loss = total_losses[1]
            winner_idx = winner    # DQN=0 in this case
        else:
            dqn_total_reward = total_rewards[1]
            dqn_loss = total_losses[1]
            pg_total_reward = total_rewards[0]
            pg_loss = total_losses[0]
            # Winner: winner=0 is PG, winner=1 is DQN
            winner_idx = (1 if winner == 1 else 0)
        if VERBOSE:
            wstr = "DQN" if winner_idx == 0 else "PG"
            print(f"[Trainer] EP {self.episode}: {wstr} wins | "
                  f"  [dqn reward/loss {dqn_total_reward:.2f}/{dqn_loss:.3f}]   "
                  f"  [pg reward/loss {pg_total_reward:.2f}/{pg_loss:.3f}]   Steps={steps}")
        return dqn_total_reward, pg_total_reward, winner_idx, steps, dqn_loss, pg_loss

    def visualize_game(self, last_episode_play=True, cur_ep=None):
        """
        Renders a live game (latest actual battle) with full visualization in Pygame.
        The game runs using the current agent policies, step-by-step.
        """
        if not self.pygame_mode:
            print("[Trainer] Skipping visualization: Pygame display not initialized.")
            return

        print("[Trainer] Visualizing live game at episode", cur_ep)
        # Set both agents to eval mode for game display: always pick highest-prob actions
        show_agents = [self.dqn, self.pg] if self.agent0_is_dqn else [self.pg, self.dqn]
        # Reset agents if necessary (no exploration)
        game = Battleship()
        obs0, obs1 = game.reset()
        states = [obs0, obs1]
        boards = [game.boards[0], game.boards[1]]
        done = False
        step = 0
        winner = None

        while not game.is_over():
            agent_idx = game.turn
            o_board, t_board = game.get_observation(agent_idx)
            legal_mask = game.legal_action_mask(agent_idx)
            action = show_agents[agent_idx].select_action(o_board, t_board, legal_mask, eval_mode=True)
            next_obs, reward, done, info = game.step(agent_idx, action)
            states[agent_idx] = next_obs
            # Render boards after each step for visual effect
            ships0 = boards[0].ships
            ships1 = boards[1].ships
            obs_p0 = game.get_observation(0)
            obs_p1 = game.get_observation(1)
            self.renderer.render_frame(
                obs_p0, obs_p1, ships0, ships1,
                turn=game.turn, winner=game.get_winner() if game.is_over() else None,
                episode=cur_ep
            )
            # Draw metrics panel as well
            dqn_stats = self.dqn.get_stats()
            pg_stats = self.pg.get_stats()
            self.metrics_viz.render(dqn_stats, pg_stats, episode_idx=cur_ep)
            self.renderer.update()
            # Add slight delay for visualization
            delay = 180 if done else 60
            pygame.time.wait(delay)
            step += 1

        # Wait for user input to continue, so they can see the result
        self.renderer.wait_for_key()

    def play_demo(self, num_games=1):
        """
        Runs a deterministic demo (play mode) between current agents, visualizing results.
        """
        if not self.pygame_mode:
            print("[Trainer] No pygame visualization - can't demo.")
            return
        for g in range(num_games):
            print(f"[Trainer] Demo game {g+1}/{num_games}")
            self.visualize_game(cur_ep="-demo-")

# ================================
# Standalone test block
# ================================
if __name__ == "__main__":
    # Standalone: run a few games with visualization to test trainer
    pygame.init()
    screen = pygame.display.set_mode((SCREEN_WIDTH, SCREEN_HEIGHT))
    dqn_agent = DQNAgent()
    pg_agent = PolicyGradientAgent()
    trainer = BattleshipTrainer(
        dqn_agent,
        pg_agent,
        screen=screen,
        visualize_every=2,
        num_episodes=4  # For test
    )
    trainer.run()
    print("[trainer.py] Standalone trainer test done. Exiting in 2 seconds...")
    pygame.time.wait(2000)
    pygame.quit()

# ================================
# Notes/Possible Improvements:
# - Could allow more agent types or random/self-play (bots).
# - Support for test/eval mode versus full training mode.
# - Better statistics aggregation (per-move, per-step stats, etc).
# - Add checkpointing: save/load agent weights every N episodes.
# - Multi-thread metrics calculation or async plot update for faster main loop.
# - Add logging to disk or TensorBoard (optional).
# ================================
```

## C:\Users\Machine81\Slazy\repo\battleshiprl\visualization\metrics.py
```python
# /home/myuser/apps/battleshiprl/visualization/metrics.py

"""
Training metrics visualization for RL agents

- Renders charts for reward, loss, win rate for DQN and Policy Gradient agents.
- Uses matplotlib to create line charts, saves as images, and blits onto Pygame panel.
- Allows easy, real-time monitoring of agent learning progress and model comparison.
"""

import numpy as np
import io
import pygame
import matplotlib
matplotlib.use("Agg")  # Non-interactive backend for headless use
import matplotlib.pyplot as plt

from game.constants import (
    SCREEN_WIDTH, SCREEN_HEIGHT, METRICS_PANEL_HEIGHT,
    BOARD_PIXEL_SIZE, BOARD_PADDING, MARGIN,
    COLOR_METRICS_BG, COLOR_METRICS_ACCENT, COLOR_TEXT,
    CURVE_DQN, CURVE_PG, CURVE_WINRATE, REWARD_AVG_WINDOW, WINRATE_AVG_WINDOW,
    FONT_NAME, FONT_SIZE_METRICS
)

class MetricsVisualizer:
    """
    Handles plotting and rendering of RL training metrics.
    Designed to be called every frame (or every N steps) to update charts/statistics.
    """
    def __init__(self, screen):
        """
        screen: Pygame surface to render metrics panel to.
        """
        self.screen = screen
        # Metrics panel region (at bottom of main screen)
        self.metrics_rect = pygame.Rect(
            0, SCREEN_HEIGHT - METRICS_PANEL_HEIGHT,
            SCREEN_WIDTH, METRICS_PANEL_HEIGHT
        )
        pygame.font.init()
        self.font = pygame.font.Font(FONT_NAME, FONT_SIZE_METRICS)
        # For optimization: cache last plot images and data to limit recomputation
        self.last_data_hash = None
        self.last_surface = None

    def get_metrics_surface(self, dqn_stats, pg_stats, episode_idx):
        """
        Generates a pygame.Surface of the current metrics plots.
        dqn_stats/pg_stats: dictionaries with 'reward', 'loss', 'winrate'
        episode_idx: current episode number
        Returns: Pygame surface (same size as self.metrics_rect)
        """
        # Turn RL stats lists into numpy arrays, use running average for smoothing
        dqn_reward = np.array(dqn_stats.get("reward", []))
        dqn_loss = np.array(dqn_stats.get("loss", []))
        dqn_winrate = np.array([dqn_stats.get("winrate", 0.0)] * len(dqn_reward)) if "reward" in dqn_stats else np.zeros(1)
        pg_reward = np.array(pg_stats.get("reward", []))
        pg_loss = np.array(pg_stats.get("loss", []))
        pg_winrate = np.array([pg_stats.get("winrate", 0.0)] * len(pg_reward)) if "reward" in pg_stats else np.zeros(1)
        # Hash of stats arrays to detect changes
        data_hash = (
            hash(dqn_reward.tobytes()) ^ hash(pg_reward.tobytes()) ^
            hash(dqn_loss.tobytes()) ^ hash(pg_loss.tobytes())
        )
        if self.last_data_hash == data_hash and self.last_surface is not None:
            return self.last_surface

        fig, axes = plt.subplots(1, 3, figsize=(12, 3), facecolor='#232432')
        fig.tight_layout(pad=4.0)
        fig.subplots_adjust(left=0.06, right=0.98, top=0.92, bottom=0.18, wspace=0.33)
        fig.patch.set_facecolor('#232432')

        # --- 1. Reward Curve ---
        ax0 = axes[0]
        ax0.set_facecolor('#252542')
        # Plot DQN
        if dqn_reward.size > 0:
            smooth_dqn = self.moving_average(dqn_reward, REWARD_AVG_WINDOW)
            ax0.plot(
                np.arange(len(smooth_dqn)), smooth_dqn,
                color=CURVE_DQN, label='DQN', linewidth=2
            )
        # Plot PG
        if pg_reward.size > 0:
            smooth_pg = self.moving_average(pg_reward, REWARD_AVG_WINDOW)
            ax0.plot(
                np.arange(len(smooth_pg)), smooth_pg,
                color=CURVE_PG, label='Policy Grad', linewidth=2
            )
        ax0.set_title("Reward (rolling avg)", color='w', fontsize=12)
        ax0.set_xlabel("Episodes", color='w', fontsize=10)
        ax0.set_ylabel("Reward", color='w', fontsize=10)
        ax0.legend(fontsize=8, facecolor='#252542')
        ax0.spines['bottom'].set_color('#b3b3b3'); ax0.spines['left'].set_color('#b3b3b3')
        ax0.tick_params(colors='w')

        # --- 2. Loss Curve ---
        ax1 = axes[1]
        ax1.set_facecolor('#252542')
        if dqn_loss.size > 0:
            smooth_dl = self.moving_average(dqn_loss, REWARD_AVG_WINDOW)
            ax1.plot(np.arange(len(smooth_dl)), smooth_dl, color=CURVE_DQN, label='DQN Loss', linewidth=2)
        if pg_loss.size > 0:
            smooth_pl = self.moving_average(pg_loss, REWARD_AVG_WINDOW)
            ax1.plot(np.arange(len(smooth_pl)), smooth_pl, color=CURVE_PG, label='PG Loss', linewidth=2)
        ax1.set_title("Loss (NN)", color='w', fontsize=12)
        ax1.set_xlabel("Episodes", color='w', fontsize=10)
        ax1.set_ylabel("Loss", color='w', fontsize=10)
        ax1.legend(fontsize=8, facecolor='#252542')
        ax1.spines['bottom'].set_color('#b3b3b3'); ax1.spines['left'].set_color('#b3b3b3')
        ax1.tick_params(colors='w')

        # --- 3. Winrate plot ---
        ax2 = axes[2]
        ax2.set_facecolor('#252542')
        if dqn_reward.size > 0:
            winrates_dqn = self.calc_running_winrate(dqn_reward, WINRATE_AVG_WINDOW)
            ax2.plot(np.arange(len(winrates_dqn)), winrates_dqn, color=CURVE_DQN, label='DQN', linewidth=2)
        if pg_reward.size > 0:
            winrates_pg = self.calc_running_winrate(pg_reward, WINRATE_AVG_WINDOW)
            ax2.plot(np.arange(len(winrates_pg)), winrates_pg, color=CURVE_PG, label='PG', linewidth=2)
        # Or simply plot the scalar winrate from stats (up to latest)
        ax2.set_ylim(-0.02, 1.05)
        ax2.set_title("Win Rate (rolling)", color='w', fontsize=12)
        ax2.set_xlabel("Episodes", color='w', fontsize=10)
        ax2.set_ylabel("Winrate", color='w', fontsize=10)
        ax2.legend(fontsize=8, facecolor='#252542', loc='lower right')
        ax2.spines['bottom'].set_color('#b3b3b3'); ax2.spines['left'].set_color('#b3b3b3')
        ax2.tick_params(colors='w')
        # Optional highlight
        ax2.axhline(0.5, linestyle=':', color='#bbbbbb', alpha=0.5)

        # --- Finalize figure: add global title with current episode ---
        fig.suptitle(
            f"BattleshipRL Training Metrics   |   Episode: {episode_idx}", color='w', fontsize=16
        )

        # Draw matplotlib fig to buffer then convert to Pygame surface
        buf = io.BytesIO()
        fig.canvas.print_png(buf)
        buf.seek(0)
        im = plt.imread(buf, format='png')
        plt.close(fig)

        # Convert to pygame surface; scale to metrics_rect size
        im_h, im_w, im_c = im.shape
        surf = pygame.image.frombuffer(
            (im * 255).astype(np.uint8).tobytes(), (im_w, im_h), "RGBA" if im_c == 4 else "RGB"
        )
        surf = pygame.transform.smoothscale(surf, (self.metrics_rect.width, self.metrics_rect.height))
        self.last_data_hash = data_hash
        self.last_surface = surf
        return surf

    def render(self, dqn_stats, pg_stats, episode_idx):
        """
        Blits metrics panel onto full screen.
        Should be called at end of main render loop to overlay metrics.
        """
        # Fill metrics rect with background color
        pygame.draw.rect(self.screen, COLOR_METRICS_BG, self.metrics_rect)
        surf = self.get_metrics_surface(dqn_stats, pg_stats, episode_idx)
        self.screen.blit(surf, self.metrics_rect.topleft)
        # Optionally, print a model-vs-model winrate summary at top left
        dqn_wr = dqn_stats.get("winrate", 0.0)
        pg_wr = pg_stats.get("winrate", 0.0)
        header_text = f"Winrate:  DQN ({dqn_wr:.2%})   |   PolicyGrad ({pg_wr:.2%})"
        txsurf = self.font.render(header_text, True, COLOR_TEXT)
        self.screen.blit(txsurf, (self.metrics_rect.left + 22, self.metrics_rect.top + 8))

    @staticmethod
    def moving_average(arr, window):
        """
        Returns array of moving averages for input arr[] (1d np.array).
        """
        if len(arr) < 1:
            return np.array([])
        if len(arr) < window:
            return arr.copy()
        cumsum = np.cumsum(np.insert(arr, 0, 0))
        return (cumsum[window:] - cumsum[:-window]) / float(window)

    @staticmethod
    def calc_running_winrate(reward_arr, window):
        """
        Returns moving average of wins per episode, inferring win from large reward jump.
        Assumes that win episodes include a large final reward (e.g., +10).
        Any reward >7 is counted as a win.
        """
        wins = np.array([1.0 if r > 7.0 else 0.0 for r in reward_arr])
        if len(wins) < 1:
            return np.array([])
        if len(wins) < window:
            return np.cumsum(wins) / np.arange(1, len(wins) + 1)
        winrate = np.convolve(wins, np.ones(window), 'valid') / window
        return winrate

# ========== Standalone test ===========
if __name__ == "__main__":
    import random
    print("[metrics.py] Running standalone metrics panel test...")
    pygame.init()
    width = SCREEN_WIDTH
    height = 330
    test_screen = pygame.display.set_mode((width, height))
    test_screen.fill((24, 29, 37))
    mv = MetricsVisualizer(test_screen)
    # Make fake stats histories
    N = 500
    dqn_rewards = np.cumsum(np.random.randn(N) * 0.8 + 0.15)
    pg_rewards = np.cumsum(np.random.randn(N) * 0.6 + 0.06)
    dqn_losses = np.abs(np.random.randn(N) * 0.1 + 0.09)
    pg_losses = np.abs(np.random.randn(N) * 0.13 + 0.12)
    dqn_stats = {
        "reward": dqn_rewards.tolist(),
        "loss": dqn_losses.tolist(),
        "winrate": random.uniform(0.40, 0.65)
    }
    pg_stats = {
        "reward": pg_rewards.tolist(),
        "loss": pg_losses.tolist(),
        "winrate": random.uniform(0.35, 0.6)
    }
    mv.render(dqn_stats, pg_stats, episode_idx=N)
    pygame.display.flip()
    print("[metrics.py] Metrics visualization rendered. Close the window or press any key to exit.")
    done = False
    while not done:
        for event in pygame.event.get():
            if event.type == pygame.QUIT:
                done = True
            elif event.type == pygame.KEYDOWN:
                done = True
    pygame.quit()

# ==========================================
# Notes/Possible Improvements:
# - For efficiency, only update figure every N episodes, or when stats change.
# - Support customizable plotting metrics (entropy, Q-values, action distributions).
# - Add per-model inset charts or highlight agent "streaks"/win/loss runs.
# - Add table with additional statistics (mean reward, avg move count, tie rate).
# - Could support direct logging to csv file for later analysis.
# - Add timeline marker indicating current episode.
# ==========================================
```

## C:\Users\Machine81\Slazy\repo\battleshiprl\visualization\renderer.py
```python
# /home/myuser/apps/battleshiprl/visualization/renderer.py

"""
Pygame renderer for Battleship game
- Visualizes two boards (player 0, player 1), their ships, and visible info.
- Draws ships, hits, misses, sunk cells, and grid; supports SpriteManager assets.
- Shows player turn info and winner.
- Called each frame from the main training/visualization loop.
"""

import pygame
import sys
from game.constants import (
    BOARD_SIZE, SHIP_NAMES, SHIP_SIZES, NUM_SHIPS,
    CELL_EMPTY, CELL_SHIP, CELL_HIT, CELL_MISS, CELL_SUNK,
    COLOR_BG, COLOR_GRID, COLOR_SHIP, COLOR_HIT, COLOR_MISS, COLOR_SUNK,
    COLOR_TEXT, COLOR_METRICS_BG, COLOR_METRICS_ACCENT, SHIP_COLORS,
    SCREEN_WIDTH, SCREEN_HEIGHT, CELL_SIZE, BOARD_PADDING, MARGIN, BOARD_PIXEL_SIZE,
    FONT_NAME, FONT_SIZE_BOARD, FONT_SIZE_METRICS
)
from assets.sprites import SpriteManager

class GameRenderer:
    def __init__(self, screen, sprite_manager=None):
        """
        screen: Pygame display surface to draw on.
        sprite_manager: optional SpriteManager for image-based display, else fallback to drawing.
        """
        self.screen = screen
        self.sprites = sprite_manager or SpriteManager()
        # Left and right board top-left x,y
        self.left_board_pos = (BOARD_PADDING, BOARD_PADDING)
        self.right_board_pos = (BOARD_PADDING + BOARD_PIXEL_SIZE + BOARD_PADDING + MARGIN, BOARD_PADDING)
        # Fonts setup
        pygame.font.init()
        self.font_board = pygame.font.Font(FONT_NAME, FONT_SIZE_BOARD)
        self.font_big = pygame.font.Font(FONT_NAME, FONT_SIZE_BOARD + 6)
        self.font_metrics = pygame.font.Font(FONT_NAME, FONT_SIZE_METRICS)
        if not pygame.display.get_init():
            pygame.display.init()
        self.status_height = 40  # px at top for info

    def draw_board(self, topleft, board_obs, show_ships=False, ships=None):
        """
        Draws a single board to 'screen' at a given topleft (x,y).
        board_obs: np.array of shape (BOARD_SIZE, BOARD_SIZE) with cell codes.
        Optionally shows ships if show_ships = True and 'ships' provided.
        """
        x0, y0 = topleft
        for y in range(BOARD_SIZE):
            for x in range(BOARD_SIZE):
                v = board_obs[y, x]
                cell_rect = pygame.Rect(
                    x0 + x * CELL_SIZE, y0 + y * CELL_SIZE, CELL_SIZE, CELL_SIZE
                )
                # Draw cell background
                pygame.draw.rect(self.screen, COLOR_BG, cell_rect)
                sprite_blitted = False

                # Ship (if showing own ships, or revealing)
                if show_ships and v in (CELL_SHIP, CELL_SUNK):
                    ship_color = COLOR_SHIP
                    if ships:
                        # Color ship by its index/type if possible
                        for idx, ship in enumerate(ships):
                            if (x, y) in ship['positions']:
                                ship_color = SHIP_COLORS[idx % len(SHIP_COLORS)]
                    # Try sprite, else fallback
                    if self.sprites and self.sprites.get_sprite("ship"):
                        ship_sprite = self.sprites.get_sprite("ship")
                        ship_surface = ship_sprite.copy()
                        if ship_color != COLOR_SHIP:
                            # Tint ship color for type
                            arr = pygame.surfarray.pixels3d(ship_surface)
                            arr[:, :, :] = ship_color
                            del arr
                        self.screen.blit(ship_surface, cell_rect)
                    else:
                        base_color = ship_color if v == CELL_SHIP else COLOR_SUNK
                        pygame.draw.rect(self.screen, base_color, cell_rect, border_radius=5)
                    sprite_blitted = True  # Using sprite or fallback graphics

                # Hit and Miss: overlay sprites or graphics
                if v == CELL_HIT:
                    if self.sprites and self.sprites.get_sprite("hit"):
                        self.screen.blit(self.sprites.get_sprite("hit"), cell_rect)
                    else:
                        pygame.draw.circle(self.screen, COLOR_HIT, cell_rect.center, CELL_SIZE // 3)
                    sprite_blitted = True
                elif v == CELL_MISS:
                    if self.sprites and self.sprites.get_sprite("miss"):
                        self.screen.blit(self.sprites.get_sprite("miss"), cell_rect)
                    else:
                        pygame.draw.circle(self.screen, COLOR_MISS, cell_rect.center, CELL_SIZE // 4)
                    sprite_blitted = True

                # Sunk but not showing as ship above: dark fill
                if v == CELL_SUNK and not show_ships:
                    pygame.draw.rect(self.screen, COLOR_SUNK, cell_rect)

                # Grid border
                pygame.draw.rect(self.screen, COLOR_GRID, cell_rect, 1)

        # Draw thick outer grid border
        board_rect = pygame.Rect(x0, y0, BOARD_SIZE * CELL_SIZE, BOARD_SIZE * CELL_SIZE)
        pygame.draw.rect(self.screen, COLOR_GRID, board_rect, 3)

    def draw_labels(self, topleft, title, who=None):
        """
        Draw player/board label above its grid.
        """
        x0, y0 = topleft
        txsurf = self.font_board.render(title, True, COLOR_TEXT)
        txrect = txsurf.get_rect(midbottom=(x0 + BOARD_SIZE*CELL_SIZE//2, y0 - 8))
        self.screen.blit(txsurf, txrect)
        if who is not None:
            whosurf = self.font_metrics.render(who, True, COLOR_METRICS_ACCENT)
            whorect = whosurf.get_rect(midtop=(x0 + BOARD_SIZE*CELL_SIZE//2, y0 + BOARD_SIZE*CELL_SIZE + 4))
            self.screen.blit(whosurf, whorect)

    def render_frame(self, obs_p0, obs_p1, ships0, ships1, turn, winner=None, episode=None):
        """
        Draws the full frame:
        - Two boards (left = player 0, right = player 1)
        - Ships, known hits/misses
        - Player turn
        - Winner message if game over
        - Top status bar: episode counter etc.
        obs_p0: (own_board0, target_board0)
        obs_p1: (own_board1, target_board1)
        ships0/ships1: for coloring ships if reveal
        turn: 0 or 1 (whose turn)
        winner: None or int
        episode: (optional) which episode to display at top
        """
        self.screen.fill(COLOR_BG)
        # Draw player 0's board on the left (always reveal for visualizer/demo)
        self.draw_board(self.left_board_pos, obs_p0[0], show_ships=True, ships=ships0)
        self.draw_labels(self.left_board_pos, "Player 0 Board", who="(Left)")
        # Draw player 1's board on the right
        self.draw_board(self.right_board_pos, obs_p1[0], show_ships=True, ships=ships1)
        self.draw_labels(self.right_board_pos, "Player 1 Board", who="(Right)")

        # Status (top): Episode, turn, winner
        status_y = 2
        status_msgs = []
        if episode is not None:
            status_msgs.append(f"Episode: {episode}")
        status_msgs.append(f"Turn: Player {turn}")
        if winner is not None:
            result_msg = f"Winner: Player {winner}!"
            status_msgs.append(result_msg)
        xmid = SCREEN_WIDTH // 2
        status_text = "    ".join(status_msgs)
        txtsurf = self.font_big.render(status_text, True, COLOR_TEXT)
        txtrect = txtsurf.get_rect(midtop=(xmid, status_y))
        self.screen.blit(txtsurf, txtrect)

        # If game is over, show large overlay
        if winner is not None:
            msg = f"PLAYER {winner} WINS!"
            winfont = pygame.font.Font(FONT_NAME, FONT_SIZE_BOARD + 20)
            surf = winfont.render(msg, True, COLOR_HIT)
            rect = surf.get_rect(center=(SCREEN_WIDTH // 2, BOARD_PADDING + BOARD_PIXEL_SIZE // 2))
            self.screen.blit(surf, rect)

    def update(self):
        """
        Flip the Pygame display buffer to show the latest frame.
        """
        pygame.display.flip()

    def wait_for_key(self):
        """
        Pauses until the user presses any key or closes the window.
        """
        waiting = True
        while waiting:
            for event in pygame.event.get():
                if event.type == pygame.QUIT:
                    pygame.quit(); sys.exit()
                elif event.type == pygame.KEYDOWN or event.type == pygame.MOUSEBUTTONDOWN:
                    waiting = False

# ================================
# Test block (standalone visualization of random boards)
# ================================
if __name__ == "__main__":
    import numpy as np
    import random

    print("[renderer.py] Testing GameRenderer...")
    pygame.init()
    screen = pygame.display.set_mode((SCREEN_WIDTH, SCREEN_HEIGHT))
    pygame.display.set_caption("BattleshipRL - Renderer Test")
    renderer = GameRenderer(screen)

    # Random test boards
    def randboard():
        b = np.zeros((BOARD_SIZE, BOARD_SIZE), dtype=np.int8)
        for i in range(7):
            x, y = random.randint(0, BOARD_SIZE-1), random.randint(0, BOARD_SIZE-1)
            b[y, x] = random.choice([CELL_SHIP, CELL_HIT, CELL_MISS, CELL_SUNK])
        return b

    # Fake ships info
    random_ships = [
        {'name': SHIP_NAMES[idx], 'size': SHIP_SIZES[idx], 'positions': [
            (random.randint(0, BOARD_SIZE-1), random.randint(0, BOARD_SIZE-1)) for _ in range(SHIP_SIZES[idx])], 'hits': set()}
        for idx in range(NUM_SHIPS)
    ]

    obs0 = (randboard(), randboard())
    obs1 = (randboard(), randboard())
    ships0 = random_ships
    ships1 = random_ships
    turn = random.choice([0, 1])
    winner = random.choice([None, 0, 1])

    # Render loop
    renderer.render_frame(obs0, obs1, ships0, ships1, turn, winner=winner, episode=123)
    renderer.update()
    print("[renderer.py] Press any key to quit viewer...")
    renderer.wait_for_key()
    pygame.quit()
    print("[renderer.py] Renderer test complete.")

# ================================
# Notes/Possible Improvements:
# - Could add hover/click to inspect cell details (debug).
# - Support animation frames for ship hit/sunk, water splash, etc.
# - Optionally display the agent's target_board somewhere for agent's knowledge view.
# - Can add buttons or legend explaining color codes.
# - Could refactor for better separation if using more board types/views (e.g., for agent-only display).
# - Use per-cell highlighting for last move/last hit for better clarity.
# ================================
```

                    
        Here is the description of the code:
        Update the BattleshipTrainer class to gracefully handle headless operation and avoid any visualization-related errors. The modifications should:
1. Add more robust checks for headless mode
2. Skip all visualization calls when no screen is present 
3. Add more detailed console output for training progress
4. Include improved error handling for visualization components
                    
        Here is the code skeleton to implement:
        None
        